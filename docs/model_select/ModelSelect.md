# æ¦‚ç‡æ ¡å‡†

## æ¦‚ç‡æ ¡å‡†åŸºæœ¬æ–¹æ³•ä¸æ¦‚å¿µ(calibration)

### åˆ†æ•°æ ¡å‡†çš„æ¦‚å¿µä¸å¯é æ€§æ›²çº¿

**åœ¨æœºå™¨å­¦ä¹ æ¨¡å‹å®è·µåº”ç”¨ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦å…³æ³¨åˆ†ç±»æ¨¡å‹çš„æ’åºæ€§(ranking)ã€‚**
è¿™å°±å¼•å‡ºäº†æ ¡å‡†(calibration)çš„æ¦‚å¿µï¼Œå³é¢„æµ‹åˆ†å¸ƒå’ŒçœŸå®åˆ†å¸ƒï¼ˆè§‚æµ‹ï¼‰åœ¨ç»Ÿè®¡ä¸Šçš„ä¸€è‡´æ€§ã€‚

> ä¸€èˆ¬æ¦‚ç‡æ ¡å‡†å¯åˆ†ä¸ºä¸‰ä¸ªæ–¹é¢:
1. **æ¬ é‡‡æ ·æ¦‚ç‡æ ¡å‡†**: ç”±äºæ­£è´Ÿæ ·æœ¬çš„é‡‡æ ·é¢‘ç‡ä¸åŒå¯¼è‡´çš„æ¨¡å‹è¾“å‡ºæ¦‚ç‡ä¸å®é™…æ¦‚ç‡ä¸ä¸€è‡´ã€‚
2. **æ¨¡å‹æ¦‚ç‡æ ¡å‡†**: ç”±äºä½¿ç”¨çš„æ¨¡å‹æ–¹æ³•ä¸åŒï¼Œç”±æ¨¡å‹ç‰¹æ€§å¯¼è‡´çš„æ¨¡å‹è¾“å‡ºæ¦‚ç‡ä¸å®é™…æ¦‚ç‡ä¸ä¸€è‡´ã€‚
3. **é”™è¯¯åˆ†é…æ¦‚ç‡æ ¡å‡†**: ç”±äºæµ‹è¯•æ ·æœ¬ä¸æ³›åŒ–æ ·æœ¬ä¹‹é—´çš„åˆ†å¸ƒå‘ç”Ÿäº†å˜åŒ–ï¼Œå¯¼è‡´æ¨¡å‹è¾“å‡ºæ¦‚ç‡ä¸å®é™…æ¦‚ç‡ä¸ä¸€è‡´ã€‚

> å¯é æ€§æ›²çº¿å›¾(`Reliability Curve Diagrams`)

ç”±äºæˆ‘ä»¬æ— æ³•è·çŸ¥çœŸå®çš„æ¡ä»¶æ¦‚ç‡ï¼Œé€šå¸¸ç”¨è§‚æµ‹æ ·æœ¬çš„æ ‡ç­¾æ¥ç»Ÿè®¡ä»£æ›¿ï¼Œ
å¹¶ç”¨å¯é æ€§æ›²çº¿å›¾(`Reliability Curve Diagrams`)æ¥ç›´è§‚å±•ç¤ºå½“å‰æ¨¡å‹çš„è¾“å‡ºç»“æœä¸çœŸå®ç»“æœæœ‰å¤šå¤§åå·®ã€‚
å¦‚ä¸‹å›¾æ‰€ç¤º:

<img src="./img/calibration.png" width="480px">

- æ¨ªåæ ‡ä¸ºäº‹ä»¶å‘ç”Ÿé¢„æµ‹æ¦‚ç‡ï¼Œ
- çºµåæ ‡ä¸ºäº‹ä»¶å‘ç”Ÿå®é™…é¢‘ç‡ï¼Œ
- å±•ç¤º`æŸä¸ªäº‹ä»¶é¢„æµ‹æ¦‚ç‡ VS å®é™…å‘ç”Ÿçš„é¢‘ç‡`ä¹‹é—´çš„å…³ç³»ã€‚
- å¯¹äºä¸€ä¸ªç†æƒ³çš„é¢„æµ‹ç³»ç»Ÿï¼Œä¸¤è€…æ˜¯å®Œå…¨ä¸€è‡´çš„ï¼Œä¹Ÿå°±æ˜¯å¯¹è§’çº¿ã€‚

å¦‚æœæ•°æ®ç‚¹å‡ ä¹éƒ½è½åœ¨å¯¹è§’çº¿ä¸Šï¼Œé‚£ä¹ˆè¯´æ˜æ¨¡å‹è¢«æ ¡å‡†å¾—å¾ˆå¥½ï¼›
åä¹‹ï¼Œå¦‚æœå’Œå¯¹è§’çº¿çš„åç¦»ç¨‹åº¦è¶Šæ˜æ˜¾ï¼Œåˆ™æ ¡å‡†è¶Šå·®ã€‚

> ç»˜åˆ¶æ–¹æ³•è¯·å‚è€ƒï¼š[å¯é æ€§æ›²çº¿å›¾](./model_select/ModelSelect?id=æ¦‚ç‡æ ¡å‡†æ›²çº¿-å¯é æ€§æ›²çº¿å›¾)

### åˆ†æ•°æ ¡å‡†çš„ä¸šåŠ¡åº”ç”¨åœºæ™¯

> åˆ†æ•°æ ¡å‡†ä¸»è¦ç›®çš„åœ¨äºï¼š

- ç¡®ä¿ä¸åŒè¯„åˆ†å¡ç»™å‡ºçš„åˆ†æ•°å…·æœ‰ç›¸åŒçš„å«ä¹‰ã€‚
- æé«˜é¢„æµ‹æ¦‚ç‡ä¸çœŸå®æ¦‚ç‡ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚
- ä¿®æ­£å®é™…æ¦‚ç‡å’Œå¼€å‘æ ·æœ¬ä¸­æœŸæœ›æ¦‚ç‡ä¹‹é—´çš„åå·®ã€‚

> ä¸šåŠ¡åœºæ™¯

**åˆ†ç¾¤è¯„åˆ†å¡**

- å•ä¸€è¯„åˆ†å¡åœ¨å…¨é‡äººç¾¤ä¸Šè¡¨ç°å¹¶ä¸æ˜¯ç‰¹åˆ«å¥½ã€‚æ­¤æ—¶ä¼šé‡‡ç”¨å…ˆåˆ†ç¾¤(`segmentation`)ï¼Œå†é’ˆå¯¹å„äººç¾¤å»ºç«‹å¤šä¸ªå­è¯„åˆ†å¡æ¨¡å‹ã€‚
åŸºäºä»¥ä¸‹å‡ ä¸ªåŸå› ï¼Œæˆ‘ä»¬éœ€è¦æŠŠåˆ†æ•°æ ¡å‡†åˆ°åŒä¸€å°ºåº¦ã€‚
    1. é’ˆå¯¹å¤šä¸ªåˆ†æ”¯æ¨¡å‹éœ€è¦åˆ¶è®¢å¤šå¥—é£æ§ç­–ç•¥ï¼Œå°†ä¼šå¤§å¤§å¢åŠ ç­–ç•¥åŒå­¦çš„å·¥ä½œé‡ï¼Œä¸”ä¸åˆ©äºç­–ç•¥ç»´æŠ¤è°ƒæ•´ã€‚
    1. ä¸åŒè¯„åˆ†å¡è¾“å‡ºçš„åˆ†æ•°å¹¶ä¸å…·æœ‰å¯æ¯”æ€§ï¼Œå®ƒä»¬çš„åˆ†å¸ƒå­˜åœ¨å·®å¼‚ã€‚ä¸ºäº†èåˆåç»Ÿä¸€è¾“å‡ºä¸€ä¸ªæœ€ç»ˆåˆ†æ•°ã€‚
    1. å„åˆ†ç¾¤è¯„åˆ†å¡ç›¸å½“äºä¸€ä¸ªåˆ†æ®µå‡½æ•°ï¼Œåˆ†æ•°ä¹‹é—´å­˜åœ¨è·ƒå˜ã€‚æ ¡å‡†å¯ä»¥ä¿è¯å„åˆ†æ•°å…·æœ‰è¿ç»­æ€§ã€‚

**é™çº§å¤‡ç”¨ç­–ç•¥**

- åœ¨ç”¨åˆ°å¤–éƒ¨æ•°æ®å»ºæ¨¡æ—¶ï¼Œè€ƒè™‘åˆ°å¤–éƒ¨æ•°æ®é‡‡é›†ä¸Šå­˜åœ¨æ½œåœ¨çš„ä¸ç¨³å®šæ€§ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šé‡‡å–é™çº§ç­–ç•¥ã€‚
ä¹Ÿå°±æ˜¯è¯´ï¼Œå»æ‰å¤–éƒ¨æ•°æ®åå†å»ºç«‹ä¸€ä¸ªæ¨¡å‹ï¼Œä½œä¸ºä¸»ç”¨(`active`)æ¨¡å‹çš„ä¸€ä¸ªå¤‡ç”¨(`standby`)æ¨¡å‹ã€‚
å¦‚æœå¤–éƒ¨æ•°æ®æœ‰ä¸€å¤©åœæ­¢æä¾›æœåŠ¡ï¼Œå°±å¯ä»¥åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹ä¸Šã€‚
- åŒæ—¶ï¼Œä¸ºäº†ä½¿ä¸‹æ¸¸ä¸šåŠ¡è°ƒç”¨æ— æ„ŸçŸ¥ï¼Œæˆ‘ä»¬ä¼šå°†ä¸»ç”¨å¤‡ç”¨æ¨¡å‹çš„åˆ†æ•°æ ¡å‡†è‡³ä¸€ä¸ªå°ºåº¦ã€‚
è¿™æ ·å°±èƒ½ä¿è¯é£æ§ç­–ç•¥åªéœ€è¦åˆ¶è®¢ä¸€å¥—cutoffæ–¹æ¡ˆï¼Œä¸”ä¸ç”¨è°ƒæ•´ï¼Œåªéœ€åšå¿…è¦çš„ç­–ç•¥åˆ‡æ¢æ—¥å¿—å’Œå‰åæ³¢åŠ¨ç›‘æ§å³å¯ã€‚

**å®¢ç¾¤å˜åŒ–ä¿®æ­£**

- å½“é¢å‘å®¢ç¾¤å‘ç”Ÿå˜åŒ–æ—¶ï¼Œå¼€å‘æ ·æœ¬ä¸æœ€è¿‘æ ·æœ¬ä¹‹é—´å­˜åœ¨åå·®(`bias`)ã€‚
å¦‚æœå¼€å‘æ ·æœ¬çš„`Odds`å¤§äºå®é™…çš„`Odds`ï¼Œé‚£ä¹ˆè®¡ç®—æ¯ä¸ªåˆ†æ•°æ®µçš„åæ ·æœ¬ç‡ï¼Œå¾—å‡ºæ¥çš„ç»“æœå°†ä¼šå¤§äºçœŸå®æƒ…å†µã€‚
- ç„¶è€Œè€ƒè™‘åˆ°å»ºæ¨¡æˆæœ¬ï¼Œæˆ‘ä»¬æœ‰æ—¶å¹¶ä¸æƒ³`refit`æ¨¡å‹ï¼Œæ­¤æ—¶å°±å¯ä»¥åˆ©ç”¨æœ€è¿‘æ ·æœ¬å¯¹è¯„åˆ†å¡è¿›è¡Œæ ¡å‡†ï¼Œä¿®æ­£åå·®ã€‚

### æ¨¡å‹æ¦‚ç‡æ ¡å‡†

> æ–¹æ³•ä¸€ï¼š`Platt scaling`ä½¿ç”¨`LR`æ¨¡å‹å¯¹æ¨¡å‹è¾“å‡ºçš„å€¼åšæ‹Ÿåˆ
> ï¼ˆå¹¶ä¸æ˜¯å¯¹`reliability diagram`ä¸­çš„æ•°æ®åšæ‹Ÿåˆï¼‰ï¼Œ
> é€‚ç”¨äº**æ ·æœ¬é‡å°‘**çš„æƒ…å½¢ï¼Œå¦‚ä¿¡è´·é£æ§åœºæ™¯ä¸­ã€‚

1. å¯¹äºæ¨¡å‹è¾“å‡ºç»“æœ`f(x)`ï¼Œå°†è¾“å‡ºé€šè¿‡`sigmoid`å˜åŒ–ï¼š
```latex
P(y=1|f)=\frac{1}{1+exp(Af(x)+B)}
```
2. ç„¶åé€šè¿‡æœ€å¤§åŒ–ä¼¼ç„¶å‡½æ•°ï¼ˆæœ€å°åŒ–å¯¹æ•°æŸå¤±å‡½æ•°ï¼‰çš„æ–¹æ³•å¯ä»¥æ±‚å¾—å‚æ•°`A, b`ï¼Œå³ï¼š
```latex
arg\ min_{A, B}\{-\sum_{i}y_i log(p_i)+(1-y_i)log(1-p_i)\}
```

3. æœ€åå¯å¾—æ ¡å‡†åçš„æ¦‚ç‡ï¼š
```latex
p_i=\frac{1}{1+exp(Af_i+b)}
```

`Platt scaling`æ˜¯ä¸€ç§å‚æ•°åŒ–æ–¹æ³•(`The parametric approach`)ï¼Œ 
ä½¿ç”¨`LR`æ¨¡å‹(`sigmoid`å‡½æ•°)å¯¹æ¨¡å‹çš„è¾“å‡ºå€¼è¿›è¡Œæ‹Ÿåˆï¼Œ
å°†æ¨¡å‹çš„åŸå§‹è¾“å‡ºå€¼æ˜ å°„ä¸ºæ¦‚ç‡å€¼ï¼ŒåŒºé—´`[0ï¼Œ1]`ã€‚å‡è®¾`f(x)`ä¸ºæ¨¡å‹çš„è¾“å‡ºå€¼ï¼Œ
ä¸Šå¼ä¸­çš„å‚æ•°`A`ï¼Œ`B`é€šè¿‡åœ¨è®­ç»ƒé›†`(f_i, y_i)`ä¸Šè¿›è¡Œæœ€å¤§ä¼¼ç„¶ä¼°è®¡è·å–ã€‚
**æ›´å¤šåº”ç”¨äºä¸åŒæ¨¡å‹çš„æ¦‚ç‡æ ¡å‡†**ã€‚

> æ–¹æ³•äºŒï¼š`Isotonic regression`åˆ™æ˜¯å¯¹`reliability diagram`ä¸­çš„æ•°æ®åšæ‹Ÿåˆï¼Œ

ä¿åºå›å½’æ˜¯ä¸€ç§éå‚å›å½’æ¨¡å‹`(nonparametric regression)`ã€‚
è¿™ç§æ–¹æ³•åªæœ‰ä¸€ä¸ªçº¦æŸæ¡ä»¶å³ï¼Œå‡½æ•°ç©ºé—´ä¸ºå•è°ƒé€’å¢å‡½æ•°çš„ç©ºé—´ã€‚
åŸºäºå¯é æ€§å›¾`(reliability diagram)`ï¼Œ
ç»™å®šæ¨¡å‹é¢„æµ‹çš„åˆ†æ•°`f_i` ï¼ŒçœŸå®çš„åˆ†æ•°`y_i`ï¼Œä¿åºå›å½’çš„æ¨¡å‹å…¬å¼ä¸ºï¼š

```latex
y_i = m(f_i)+\varepsilon
```

å…¶ä¸­ï¼Œ`m`è¡¨ç¤ºä¿åºå‡½æ•°ï¼ˆå•è°ƒé€’å¢å‡½æ•°ï¼‰ã€‚æ‰€ä»¥ä¿åºå›å½’çš„ç›®æ ‡å‡½æ•°ä¸º:

```latex
\hat{m} = arg\ min_z\sum(y_i - z(f_i))^2
```

âš ï¸ æ³¨æ„ï¼Œé€‚ç”¨äº**æ ·æœ¬é‡å¤š**çš„æƒ…å½¢ã€‚ä¾‹å¦‚æœç´¢æ¨èåœºæ™¯ã€‚æ ·æœ¬é‡å°‘æ—¶ï¼Œä½¿ç”¨`isotonic regression`å®¹æ˜“è¿‡æ‹Ÿåˆã€‚

### æ¬ é‡‡æ ·æ¦‚ç‡æ ¡å‡† ğŸ”¨

> æ–¹æ³•ä¸‰ï¼šæ¬ é‡‡æ ·ä¸‹çš„æ¦‚ç‡æ ¡å‡†æ–¹æ³•`Prior Correction`

å¯¹äºæ­£è´Ÿæ ·æœ¬ä¸å¹³è¡¡çš„æ ·æœ¬ï¼Œæ¬ é‡‡æ ·æ˜¯ä¸€ç§å¸¸ç”¨çš„æŠ€æœ¯ï¼Œå¯ä»¥å‡å°‘æ­£è´Ÿæ ·æœ¬çš„å€¾æ–œã€‚
ä½†æ˜¯ï¼Œç”±äºæ¬ é‡‡æ ·ä¿®æ”¹äº†è®­ç»ƒé›†çš„å…ˆéªŒåˆ†å¸ƒï¼Œå› æ­¤åŸºäºé‡‡æ ·åæ ·æœ¬è®­ç»ƒçš„åˆ†ç±»å™¨å¾—åˆ°çš„åéªŒæ¦‚ç‡ä¼šå‡ºç°åå·®ã€‚
å°½ç®¡ï¼Œæ¬ é‡‡æ ·å¯¼è‡´çš„åå·®ä¸ä¼šå½±å“åéªŒæ¦‚ç‡çš„æ’åºæ€§ï¼Œä½†æ˜¯ä¼šæ˜¾è‘—å½±å“åˆ†ç±»å¾—åˆ°çš„å‡†ç¡®æ€§å’Œä»¥åŠæ ·æœ¬çš„çœŸå®æ¦‚ç‡ã€‚

`Prior Correction`æ–¹æ³•å¯ä»¥ç†è§£æˆæ˜¯åŸºäºå…ˆéªŒåˆ†å¸ƒçš„æ ¡éªŒã€‚å¯¹è´Ÿæ ·æœ¬è¿›è¡Œæ¬ é‡‡æ ·ä¼šæ”¹å˜æ ·æœ¬çš„å…ˆéªŒåˆ†å¸ƒï¼Œ
å¯¼è‡´è®­ç»ƒå¾—åˆ°çš„æ¨¡å‹æ˜¯æœ‰åçš„ï¼Œå¹¶ä¸”å¯¹æ­£æ ·æœ¬çš„ä¼°è®¡æ¦‚ç‡åé«˜ï¼Œå› æ­¤éœ€è¦æŠŠæ­£æ ·æœ¬æ¯”ä¾‹ä½œä¸ºå…ˆéªŒä¿¡æ¯ï¼Œ
å¯¹è´Ÿé‡‡æ ·åè®­ç»ƒå¾—åˆ°çš„æ¨¡å‹è¿›è¡Œæ ¡å‡†ã€‚

åœ¨æ ·æœ¬ä¸å¹³è¡¡çš„äºŒåˆ†ç±»ä»»åŠ¡ä¸­ä½¿ç”¨æ¬ é‡‡æ ·çš„æ–¹æ³•è®­ç»ƒåˆ†ç±»å™¨ï¼Œ
åˆ™å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å…¬å¼å°†åˆ†ç±»å™¨å¾—åˆ°çš„é¢„æµ‹æ¦‚ç‡æ ¡å‡†åˆ°çœŸå®æ¦‚ç‡ä¸Šï¼š

```latex
p = \frac{wp_s}{wp_s-p_s+1}
```

å…¶ä¸­ï¼Œ `w` æ˜¯æ¬ é‡‡æ ·è¿‡ç¨‹ä¸­è´Ÿæ ·æœ¬ï¼ˆå¤šç±»æ ·æœ¬ï¼‰çš„é‡‡æ ·ç‡ï¼Œ `p_s` æ˜¯æ¬ é‡‡æ ·ååˆ†ç±»å™¨é¢„æµ‹çš„æ¦‚ç‡ã€‚

**è¯æ˜ï¼š**

å‡è®¾ç»™å®šç±» $y$ æ—¶ï¼Œæ ·æœ¬é€‰æ‹©å˜é‡ $s$ ä¸è¾“å…¥ $x$ 
æ— å…³ï¼ˆ $s=1$ è¡¨ç¤ºæ ·æœ¬è¢«é€‰ä¸­ï¼Œ$s=0$ è¡¨ç¤ºæ ·æœ¬æ²¡è¢«é€‰ä¸­ï¼‰ï¼Œ
å³ $p(s|y, x) = p(s|y)$ï¼Œè¿™ä¸ªå‡è®¾æ„å‘³ç€ $p(x|y, s) = p(x|y)$ï¼Œå³é€šè¿‡æ¶ˆé™¤å¤šæ•°ç±»ä¸­çš„éšæœºè§‚å¯Ÿå€¼ï¼Œ
ä¸ä¼šæ”¹å˜ç±»å†…è¾“å…¥ $x$ çš„åˆ†å¸ƒã€‚åŒæ—¶ï¼Œå› ä¸ºæ¬ é‡‡æ ·çš„åŸå› ï¼Œæ ·æœ¬çš„å…ˆéªŒæ¦‚ç‡å‘ç”Ÿäº†æ”¹å˜
$
p(y|s=1) \neq p
$
ï¼Œå¹¶ä¸”æ¡ä»¶æ¦‚ç‡ä¹Ÿå‘ç”Ÿäº†æ”¹å˜ï¼Œå³
$
p(y|x, s=1) \neq p(y|x)
$
ã€‚å¯¹äºé‡‡æ ·ååœ¨è®­ç»ƒé›†ä¸­çš„æ ·æœ¬ç‚¹ $(x,\\ y)$ï¼Œ
æˆ‘ä»¬ç”¨ç¬¦å· $+$ è¡¨ç¤º $y=1$ï¼Œç¬¦å· $-$ è¡¨ç¤º $y=0$ï¼Œåˆ©ç”¨è´å¶æ–¯å…¬å¼ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š

$$
\begin{aligned}
P(+|x,\ s=1)\ & =\ \frac{p(+,\ x,\ s=1)}{p(x,\ s=1)}\\\\
& =\ \frac{p(s=1|+,\ x)p(+,\ x)}{p(s=1|x)p(x)}\\\\
& =\ \frac{p(s=1|+)p(+|x)p(x)}{(p(s=1|+)p(+|x)\ +\ p(s=1|-)P(-|x))p(x)}\\\\
& =\ \frac{p(s=1|+)p(+|x)}{p(s=1|+)p(+|x)\ +\ p(s=1|-)p(-|x)}\\\\
\end{aligned}
$$
$$
\because\ P(s=1|+)\ =\ 1\\\\
P(+|x,\ s=1)\ =\ \frac{p(+|x)}{p(+|x)\ +\ p(s=1|-)p(-|x)}
$$

å®šä¹‰ $w=p(s=1|-)$ ä¸ºè´Ÿç±»æ ·æœ¬çš„é‡‡æ ·ç‡ï¼Œ$p=p(+|x)$ ä¸ºåŸå§‹æ ·æœ¬ä¸‹æ­£ç±»çš„åéªŒæ¦‚ç‡ï¼ˆå³æ ·æœ¬ä¸ºæ­£çš„çœŸå®æ¦‚ç‡ï¼‰ï¼Œ
$p_s=p(+|x,\ s=1)$ ä¸ºé‡‡æ ·åæ­£ç±»çš„åéªŒæ¦‚ç‡ï¼ˆå³åˆ©ç”¨é‡‡æ ·åæ ·æœ¬è®­ç»ƒåˆ†ç±»å™¨å¾—åˆ°çš„é¢„æµ‹æ¦‚ç‡ï¼‰ï¼Œ
åˆ™ä¸Šå¼å¯ä»¥å†™æˆï¼š
```latex
p_s = \frac{p}{p+w(1-p)}
```
å³:
```latex
p = \frac{wp_s}{wp_s-p_s+1}
```
---
*ps:*
è¯¥æ¦‚ç‡æ ¡å‡†å…¬å¼çš„å¦å¤–ä¸€ç§è¡¨ç°å½¢å¼å¦‚ä¸‹ï¼š
```latex
p(+|x)=[1+(\frac{1}{p(+|x,\ s=1)}-1)(\frac{1-\tau}{\tau})(\frac{\overline{y}}{1-\overline{y}})]^{-1}
```
å…¶ä¸­ï¼Œ`p(y=1|x)`è¡¨ç¤ºæ­£æ ·æœ¬çš„çœŸå®æ¦‚ç‡ï¼Œp(y=1|x,\ s=1)è¡¨ç¤ºæ¬ é‡‡æ ·åæ¨¡å‹é¢„æµ‹çš„æ­£æ ·æœ¬æ¦‚ç‡ï¼Œ$\tau$ 
ä¸ºé‡‡æ ·å‰æ ·æœ¬ä¸­æ­£æ ·æœ¬æ¯”ä¾‹ï¼Œ$\overline{y}$ ä¸ºé‡‡æ ·åæ ·æœ¬ä¸­æ­£æ ·æœ¬æ¯”ä¾‹ï¼Œ
å…·ä½“è¯æ˜æ–¹æ³•å¯ä»¥å‚è€ƒè®ºæ–‡<sup>[9]</sup>ã€‚
ä»¤$p$è¡¨ç¤ºçœŸå®æ¦‚ç‡ï¼ˆå³æ ¡å‡†åçš„æ¦‚ç‡ï¼‰ï¼Œ$p_s$ è¡¨ç¤ºé¢„æµ‹æ¦‚ç‡ï¼Œ$w$ è¡¨ç¤ºè´Ÿæ ·æœ¬çš„é‡‡æ ·ç‡ï¼Œå¯çŸ¥:
```latex
\frac{1}{w} = (\frac{1-\tau}{\tau})(\frac{\overline{y}}{1-\overline{y}})\\
\ \\
p = \frac{1}{1+(\frac{1}{p_s}-1)/w} = \frac{wp_s}{wp_s-p_s+1}
```
æ‰€ä»¥è¯¥æ¦‚ç‡æ ¡å‡†å…¬å¼çš„ä¸¤ç§å½¢å¼æ˜¯ç­‰ä»·çš„ã€‚
å¯¹äºäºŒåˆ†ç±»é—®é¢˜ä¸­çš„é€»è¾‘å›å½’æ¨¡å‹:
```latex
IF:\ p(y=1|x,\ s=1)=(1+e^{-X\beta})^{-1}\\
\ \\
SO:\ p(y=1|x)=[1+e^{-X\beta+ln[(\frac{1-\tau}{\tau})(\frac{\overline{y}}{1-\overline{y}})]}]^{-1}=[1+e^{-x\beta-ln(w)}]^{-1}
```
---
æ‰€ä»¥æ¬ é‡‡æ ·ååˆ©ç”¨`MLE`ä¼°è®¡å¾—åˆ°çš„æ¨¡å‹å‚æ•°ä¸­ï¼Œ
éæˆªè·é¡¹çš„å‚æ•°ä¸çœŸå®å‚æ•°æ˜¯ä¸€è‡´çš„ï¼Œä¸ç”¨æ ¡å‡†ã€‚
åªéœ€å¯¹æˆªè·é¡¹çš„å‚æ•°æŒ‰ä¸‹é¢çš„å…¬å¼è¿›è¡Œæ ¡å‡†å³å¯ï¼š
```latex
\beta_0 = \hat{\beta_0}-ln[(\frac{1-\tau}{\tau})(\frac{\overline{y}}{1-\overline{y}})] = \hat{\beta_0} + ln(w)
```
å¯¹äºLRæ¨¡å‹æ¥è¯´ï¼Œæˆªè·é¡¹çš„æ ¡å‡†å¯ä»¥ç¦»çº¿å®Œæˆï¼Œä¸éœ€è¦å®æ—¶è®¡ç®—ï¼Œèƒ½å‡å°‘å®æ—¶è®¡ç®—é‡ã€‚

> æ–¹æ³•å››ï¼šæ¬ é‡‡æ ·ä¸‹çš„æ¦‚ç‡æ ¡å‡†æ–¹æ³•`Weighting`

`Weghting`æ–¹æ³•å¯ä»¥ç†è§£æˆå¯¹æ ·æœ¬è¿›è¡ŒåŠ æƒã€‚
åœ¨è¿›è¡Œæ¬ é‡‡æ ·åï¼Œå¯¹æ­£è´Ÿæ ·æœ¬è¿›è¡Œæ ·æœ¬åŠ æƒï¼Œç„¶åå†è®­ç»ƒæ¨¡å‹ã€‚
åœ¨é€»è¾‘å›å½’æ¨¡å‹ä¸­ï¼Œå½“æ ·æœ¬ä¸åŠ æƒæ—¶ï¼Œå¯¹æ•°ä¼¼ç„¶å‡½æ•°`log-likelihood`å¯ä»¥å†™æˆï¼š

$$
\begin{aligned}
lnL(\beta|y) & = \sum_{y_{i}=1}ln(p_i)+\sum_{y_{i}=0}ln(1-p_i) \\\\
& = -\sum_{i=1}^{n}ln(1+e^{(1-2y_i)X_i\beta})
\end{aligned}
$$

å½“æ­£è´Ÿæ ·æœ¬åŠ æƒåï¼ŒåŠ æƒçš„å¯¹æ•°ä¼¼ç„¶å‡½æ•°å¯ä»¥å†™æˆï¼š

$$
\begin{aligned}
lnL_{w}(\beta|y) & = w_1\sum_{y_{i}=1}ln(p_i)+w_0\sum_{y_{i}=0}ln(1-p_i) \\\\
& = -\sum_{i=1}^{n}w_iln(1+e^{(1-2y_i)X_i\beta})
\end{aligned}
$$

$$
w_1 = \frac{\tau}{\overline{y}} = 1
$$

ä¸ºæ­£æ ·æœ¬çš„æƒé‡ï¼Œ
$$
\begin{aligned}
w_0 & = \frac{1-\tau}{1-\overline{y}} \\\\
& = (\frac{1-\tau}{\tau})(\frac{\overline{y}}{1-\overline{y}}) \\\\
& = \frac{1}{w}
\end{aligned}
$$

æ­£æ ·æœ¬æƒé‡ä¸ºè´Ÿæ ·æœ¬é‡‡æ ·ç‡çš„å€’æ•°ã€‚å¯¹æ¯”`Prior Correction`å’Œ`Weighting`æ–¹æ³•ï¼Œ
å½“å»ºç«‹çš„æ¨¡å‹å­˜åœ¨åå·®æ—¶ï¼Œ`Weighting`æ–¹æ³•çš„é²æ£’æ€§ä¼˜äº`Prior Correction`ã€‚
å½“æ ·æœ¬é‡è¾ƒå°‘æ—¶ï¼Œ`Weighting`æ–¹æ³•ä¸å¦‚`Prior Correction`ï¼Œä½†æ˜¯å·®å¼‚ä¸å¤§ï¼›
å½“æ ·æœ¬é‡è¾ƒå¤§æ—¶ï¼Œ`Weighting`æ–¹æ³•æ›´é€‚åˆã€‚è®ºæ–‡<sup>[6]</sup>çš„é™„å½•Bè¿˜æ¨å¯¼ç»™å‡ºäº†å¤šç±»åˆ«çš„æ ¡å‡†å…¬å¼ï¼Œ
è¿™ä¸ªå°±å¯ä»¥ç”¨äº`Softmax Regression`ç­‰æ¨¡å‹ä¸­ã€‚

### é”™è¯¯åˆ†é…æ¦‚ç‡æ ¡å‡† ğŸ”¨

> æ–¹æ³•äº”: Oddsæ ¡å‡†ï¼Œé”™è¯¯åˆ†é…`Misassignment`

LRä¸­çš„æˆªè·è¿‘ä¼¼äºå¼€å‘æ ·æœ¬çš„ln(Odds)ï¼Œé‚£ä¹ˆå°±å¯ä»¥é‡‡å–ä»¥ä¸‹æ–¹å¼è¿›è¡Œæ ¡å‡†ã€‚

$$
\begin{aligned}
ln(\frac{p}{1-p}) & = W^TX + ln(Odds_{expect}) \\\\
ln(Odds_{calibrate}) & = ln(\frac{Odds_{actual}}{Odds_{expect}}) = ln(Odds_{actual}) - ln(Odds_{expect})
\end{aligned}
$$

$$
\begin{aligned}
ln(\frac{p}{1-p}) & = w_1x_1 + ln(Odds_{expect}) + ln(Odds_{calibrate})\\\\
& = w_1x_1 + ln(Odds_{actual}) \\\\
\because Score & = A-Bln(Odds)
\end{aligned}
$$

é‚£ä¹ˆï¼Œåˆ©ç”¨è¿‘æœŸæ ·æœ¬å’Œå¼€å‘æ ·æœ¬å°±å¯ä»¥åˆ†åˆ«ç»˜åˆ¶å‡ºè¿™æ ·ä¸€æ¡ç›´çº¿ã€‚
å¦‚æœè¿™ä¸¤æ¡ç›´çº¿æ˜¯å¹³è¡Œå…³ç³»ï¼Œæ­¤æ—¶æˆ‘ä»¬è®¤ä¸ºï¼šåœ¨åŒä¸€ä¸ªåˆ†æ•°æ®µä¸Šï¼Œ
å¼€å‘æ ·æœ¬ç›¸å¯¹äºè¿‘æœŸæ ·æœ¬æŠŠ$Odds$é¢„ä¼°å¾—è¿‡å¤§æˆ–è¿‡å°ã€‚
å› æ­¤ï¼Œå¯é€šè¿‡ $Ln(Odds_{actual}) - Ln(Odds_{expect})$ æ¥è¿›è¡Œæ ¡æ­£ã€‚

> é’ˆå¯¹**åˆ†ç¾¤è¯„åˆ†å¡å’Œé™çº§å¤‡ç”¨ç­–ç•¥**è¿›è¡ŒPlattæ ¡å‡†

â€¼ï¸ æ³¨æ„`Platt Scaling`åº”ç”¨äºä»¥ä¸Šä¸¤ç§åœºæ™¯æ˜¯[ä¿¡ç”¨è¯„åˆ†å¡æ¨¡å‹åˆ†æ•°æ ¡å‡†](https://zhuanlan.zhihu.com/p/82670834)<sup>[5]</sup>
è¿™ç¯‡æ–‡çŒ®ç»™å‡ºçš„å»ºè®®ï¼Œä½†ä¸€èˆ¬è®¤ä¸º`Platt Scaling`æ–¹æ³•åªèƒ½åº”ç”¨äºåˆ†ç¾¤è¯„åˆ†å¡æˆ–é™çº§å¤‡ç”¨ç­–ç•¥ä¸­é‡‡ç”¨äº†ä¸åŒé¢„æµ‹æ¨¡å‹çš„æƒ…å†µã€‚
åœ¨è¿™ç§æƒ…å†µä¸‹ä½¿ç”¨`Platt Scaling`æ–¹æ³•ï¼Œå¯¹è¾“å‡ºæ¦‚ç‡è¿›è¡Œä¸€è‡´æ€§æ ¡å‡†ï¼Œå¯å¾—åˆ°ä¸€è‡´æ€§çš„æ¦‚ç‡è¾“å‡ºï¼Œä»è€Œä½¿ä¸åŒæ¨¡å‹è¾“å‡ºçš„æ¦‚ç‡å¯ä»¥è¿›è¡Œå¹³è¡Œæ¯”è¾ƒã€‚
ä½†åœ¨æ–‡çŒ®[ä¿¡ç”¨è¯„åˆ†å¡æ¨¡å‹åˆ†æ•°æ ¡å‡†](https://zhuanlan.zhihu.com/p/82670834)<sup>[5]</sup>ç»™å‡ºçš„ç¤ºä¾‹æƒ…å†µä¸‹ï¼Œ
ä¸å»ºè®®é‡‡ç”¨`Platt Scaling`æ–¹æ³•ï¼Œè€Œåº”è¯¥é‡‡ç”¨ç»Ÿä¸€çš„ç¼©æ”¾æœºåˆ¶ã€‚è¿™é‡Œ`Platt Scaling`æ–¹æ³•çš„åº”ç”¨æ ¸å¿ƒåº”è¯¥åœ¨äºï¼Œä¸åŒçš„æ¨¡å‹æ˜¯å¦é‡‡ç”¨äº†åŒæ ·çš„é¢„æµ‹æ¨¡å‹ã€‚
å› æ­¤ä¸‹æ–‡ç»™å‡ºä¸€ä¸ªç¤ºä¾‹è¿›è¡Œä¸åŒæ¨¡å‹ä¹‹é—´çš„`Platt Scaling`ï¼Œè¯·å‚è€ƒä¸‹æ–‡[*æ¨¡å‹æ¦‚ç‡æ ¡å‡†*](./model_select/ModelSelect?id=æ¨¡å‹æ¦‚ç‡æ ¡å‡†)ã€‚

> é’ˆå¯¹**å®¢ç¾¤å˜åŒ–ä¿®æ­£**è¿›è¡ŒOddsæ ¡å‡†

## æ¦‚ç‡æ ¡å‡†æ›²çº¿ | å¯é æ€§æ›²çº¿å›¾

> é—®é¢˜ä¸æ¦‚å¿µ - æ˜ç¡®ä¸€èˆ¬æœºå™¨å­¦ä¹ æ¨¡å‹çš„è¿‘ä¼¼æ¦‚ç‡è¾“å‡ºä¸é€»è¾‘å›å½’çš„å·®å¼‚

é€»è¾‘å›å½’æœ¬èº«å…·æœ‰è‰¯å¥½çš„æ ¡å‡†åº¦ï¼Œå…¶è¾“å‡ºæ¦‚ç‡ä¸çœŸå®æ¦‚ç‡ä¹‹é—´å­˜åœ¨è‰¯å¥½çš„ä¸€è‡´æ€§ã€‚
å› æ­¤ï¼Œæˆ‘ä»¬ä¹Ÿå°±å¯ä»¥ç›´æ¥æŠŠæ¦‚ç‡åˆ†æ•°çº¿å½¢æ˜ å°„ä¸ºæ•´æ•°åˆ†æ•°ã€‚

ä½†æ˜¯ï¼Œé€»è¾‘å›å½’ç®€å•çš„æ¨¡å‹ç»“æ„ä¹Ÿå¯¼è‡´äº†åœ¨å¤§æ•°æ®çš„èƒŒæ™¯ä¸‹ï¼Œå…¶æ¦‚ç‡æ‹Ÿåˆç²¾åº¦å·®å¼ºäººæ„ã€‚
å¦‚æœæˆ‘ä»¬ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚XGBoostã€éšæœºæ£®æ—ç­‰ï¼‰æ¥é£æ§å»ºæ¨¡ï¼Œ
åˆå¸Œæœ›æŠŠæ¦‚ç‡å¯¹æ ‡åˆ°çœŸå®æ¦‚ç‡ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±éœ€è¦å¯¹æœºå™¨å­¦ä¹ æ¨¡å‹çš„è¾“å‡ºç»“æœè¿›è¡Œæ¦‚ç‡æ ¡æ­£ï¼Œ
ä½¿å…¶è¾“å‡ºæ¦‚ç‡ä¸çœŸå®æ¦‚ç‡å°½é‡ä¸€è‡´ã€‚é¦–å…ˆæˆ‘ä»¬éœ€è¦æ˜ç¡®ä¸€èˆ¬æœºå™¨å­¦ä¹ æ¨¡å‹çš„è¿‘ä¼¼æ¦‚ç‡è¾“å‡ºä¸é€»è¾‘å›å½’çš„å·®å¼‚ã€‚
è¿™é‡Œé‡‡ç”¨æ¦‚ç‡æ ¡å‡†æ›²çº¿çš„æ–¹å¼ï¼Œå¯è§†åŒ–è¿™ç§å·®å¼‚ã€‚

> ç¤ºä¾‹ - æ¦‚ç‡æ ¡å‡†æ›²çº¿

```python
from kivi.LoadData import MakeData
from kivi.ModelSelect import Calibration

# åˆ›å»ºæ•°æ®é›†
make_data = MakeData()
X_train, X_test, y_train, y_test = make_data.get_dataset()

# å®ä¾‹åŒ– Calibration
calibra = Calibration()

from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC

# å®šä¹‰åˆ†ç±»å™¨
lr = LogisticRegression()
gnb = GaussianNB()
svc = LinearSVC(C=1.0)
rfc = RandomForestClassifier()

# å‚¨å­˜é¢„æµ‹æ¦‚ç‡çš„List
ProbPosLs = []

# è®­ç»ƒä¸é¢„æµ‹
for clf, name in [(lr, 'Logistic'),
                  (gnb, 'Naive Bayes'),
                  (svc, 'Support Vector Classification'),
                  (rfc, 'Random Forest')]:
  clf.fit(X_train, y_train)
  if hasattr(clf, "predict_proba"):
    prob_pos = clf.predict_proba(X_test)[:, 1]
  else:
    prob_pos = clf.decision_function(X_test)
    prob_pos = (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())
  ProbPosLs.append((name, prob_pos))

# ç»˜åˆ¶æ¦‚ç‡æ ¡å‡†æ›²çº¿
% matplotlib
inline  # ä»…åœ¨Jupyter Notebookä¸­éœ€è¦
calibra.Plot(y_test, ProbPosLs)
```

<img src="./img/calibration.png" width="480px">


> æ¦‚ç‡æ ¡å‡†åˆ†æ

- å¯¹äºä¸€èˆ¬çš„é€»è¾‘å›å½’å‡½æ•°åˆ†ç±»å™¨ï¼Œä¸€èˆ¬è¾“å‡ºå¯ä»¥ç›´æ¥è§£é‡Šä¸ºç½®ä¿¡åº¦ã€‚
ä¾‹å¦‚ï¼Œæ¨¡å‹è®¡ç®—æ ·æœ¬ä¸­ç»™å‡ºçš„`predict_proba`å€¼æ¥è¿‘äº0.8ï¼Œè¿‘ä¼¼`80ï¼…`å±äº`1`åˆ†ç±»ã€‚
- ç”±äº`LogisticRegression`ç›´æ¥ä¼˜åŒ–äº†å¯¹æ•°æŸå¤±ï¼Œå› æ­¤è¿”å›äº†ç»è¿‡è‰¯å¥½æ ¡å‡†çš„é¢„æµ‹æ¦‚ç‡ã€‚
è€Œå…¶ä»–æœºå™¨å­¦ä¹ æ–¹æ³•è¿”å›æœ‰åæ¦‚ç‡ï¼Œæ¯ç§æ–¹æ³•æœ‰ä¸åŒçš„åå·®ï¼š
- `GaussianNaiveBayes`å€¾å‘äºå°†æ¦‚ç‡æ¨ç†åˆ°`0`æˆ–`1`ï¼ˆè¯·æ³¨æ„ç›´æ–¹å›¾ä¸­çš„è®¡æ•°ï¼‰ã€‚
è¿™ä¸»è¦æ˜¯å› ä¸ºå®ƒå‡å®šè¦ç´ åœ¨ç»™å®šç±»åˆ«çš„æƒ…å†µä¸‹æ˜¯æ¡ä»¶ç‹¬ç«‹çš„ï¼Œ
- `RandomForestClassifier`è¡¨ç°å‡ºç›¸åçš„è¡Œä¸ºï¼šç›´æ–¹å›¾æ˜¾ç¤ºçš„å³°å€¼çº¦ä¸ºã€‚
æ¦‚ç‡ä¸º`0.2`å’Œ`0.9`ï¼Œè€Œæ¥è¿‘`0`æˆ–`1`çš„æ¦‚ç‡éå¸¸ç½•è§ã€‚
- æ”¯æŒå‘é‡åˆ†ç±»`SVC`ä¸`RandomForestClassifier`ç›¸æ¯”ï¼Œæ˜¾ç¤ºå‡ºæ›´å¤§çš„`S`å‹æ›²çº¿ï¼Œ
è¯¥æ–¹æ³•ä¾§é‡äºé è¿‘å†³ç­–è¾¹ç•Œçš„æ ·æœ¬ã€‚

## æ¦‚ç‡æ ¡å‡†è¯„ä¼°æŒ‡æ ‡

> å¯¹æ•°æŸå¤±å‡½æ•°(`Logarithmic Lossï¼ŒLL`)

```latex
Loss_{log} = -\sum_{i=1}^{N}(y_ilog(p_i)+(1-y_i)*log(1-p_i)) \\
\ \\
y_i \in \{0, 1\}: Actual\ class\ label \\
\ \\
p_i \in [0, 1]: Estimated\ default\ probability
```

å½“çœŸå®label=0ï¼Œé¢„æµ‹æ¦‚ç‡ä¸º1æ—¶ï¼ŒæŸå¤±å‡½æ•°å°†è¾¾åˆ°+âˆã€‚
LLæƒ©ç½šæ˜æ˜¾é”™è¯¯çš„åˆ†ç±»ã€‚å½“é¢„æµ‹æ¦‚ç‡è¶Šæ¥è¿‘äºçœŸå®æ ‡ç­¾ï¼Œ
LLè¶Šå°ï¼Œæ¨¡å‹çš„æ ¡å‡†æ•ˆæœå°±è¶Šå¥½ã€‚

> Brieråˆ†æ•°(`Brier scoreï¼ŒBS`)

```latex
Brier = \frac{1}{N}\sum_{i=1}^{N}(y_i-P_i)^2\\
\ \\
y_i \in \{0, 1\}: Actual\ class\ label \\
\ \\
p_i \in [0, 1]: Estimated\ default\ probability
```

å½“BSæŒ‡æ ‡è¶Šå°ï¼Œä»£è¡¨æ¨¡å‹é¢„æµ‹ç»“æœè¶Šæ¥è¿‘äºçœŸå®æ ‡ç­¾ã€‚
å› æ­¤ï¼Œè¿™ä¸¤ä¸ªæŒ‡æ ‡éƒ½åæ˜ æ ·æœ¬é›†ä¸ŠçœŸå®æ ‡ç­¾ä¸é¢„æµ‹æ¦‚ç‡ä¹‹é—´çš„å·®å¼‚æ€§ï¼Œä¹Ÿå°±æ˜¯ä¸€è‡´æ€§ã€‚
è¿™é‡Œå…³äº`Brier`åˆ†æ•°çš„è®¡ç®—æ¨èç›´æ¥ä½¿ç”¨`sklearn.metrics.brier_score_loss`æ–¹æ³•ã€‚
å‚æ•°æ–¹æ³•ä¸ç¤ºä¾‹å¦‚ä¸‹ã€‚

> Coding by Scikit-learn

**å‚æ•°**

```python
Parameters:
    # True targets.
    y_true: array, shape (n_samples,)
    
    # Probabilities of the positive class.
    y_prob: array, shape (n_samples,)
    
    # Sample weights.
    sample_weight: array-like of shape (n_samples,), default=None
    
    # Label of the positive class. 
    # Defaults to the greater label unless y_true is 
    # all 0 or all -1 in which case pos_label defaults to 1.
    pos_label: int or str, default=None

Returns:
    # Brier score
    score: float
```

**ç¤ºä¾‹**

```python
import numpy as np
from sklearn.metrics import brier_score_loss

y_true = np.array([0, 1, 1, 0])
y_true_categorical = np.array(["spam", "ham", "ham", "spam"])
y_prob = np.array([0.1, 0.9, 0.8, 0.3])
brier_score_loss(y_true, y_prob)
>>> 0.037...
brier_score_loss(y_true, 1-y_prob, pos_label=0)
>>> 0.037...
brier_score_loss(y_true_categorical, y_prob, pos_label="ham")
>>> 0.037...
brier_score_loss(y_true, np.array(y_prob) > 0.5)
>>> 0.0
```

## æ¨¡å‹æ¦‚ç‡æ ¡å‡†æ–¹æ³•

### æ¨¡å‹æ¦‚ç‡æ ¡å‡†æ–¹æ³•

> é—®é¢˜æè¿°

åœ¨ä¸Šé¢[æ¦‚ç‡æ ¡å‡†æ›²çº¿ | å¯é æ€§æ›²çº¿å›¾](./model_select/ModelSelect?id=æ¦‚ç‡æ ¡å‡†æ›²çº¿-å¯é æ€§æ›²çº¿å›¾)çš„ä»‹ç»ä¸­ï¼Œåˆæ­¥äº†è§£äº†é€»è¾‘å›å½’çš„æ¦‚ç‡è¾“å‡ºä¸
å…¶ä»–æœºå™¨å­¦ä¹ æ¨¡å‹æ¦‚ç‡è¾“å‡ºçš„æ¦‚ç‡å·®åˆ«ï¼›å¹¶åŸºæœ¬è®¤ä¸ºï¼Œé€»è¾‘å›å½’åŸºæœ¬è¿‘ä¼¼çœŸå®æ¦‚ç‡ï¼Œè€Œå…¶ä»–æ¨¡å‹çš„è¾“å‡ºå¹¶ä¸æ˜¯çœŸå®æ¦‚ç‡çš„è¿‘ä¼¼ã€‚

> ç»˜åˆ¶[æ¦‚ç‡æ ¡å‡†æ›²çº¿ | å¯é æ€§æ›²çº¿å›¾](./model_select/ModelSelect?id=æ¦‚ç‡æ ¡å‡†æ›²çº¿-å¯é æ€§æ›²çº¿å›¾)ä¸­çš„å„ç±»åˆ†ç±»å™¨çš„æ¦‚ç‡è¾“å‡ºåˆ†å¸ƒã€‚

```python
from kivi.kivi.FeatureSelect import KS

ks = KS()

ks.Distribution(
  dict(ProbPosLs),
)
```

**Result**

<img src="./img/calibration_distribution.png">

ä¸Šå›¾å±•ç¤ºäº†ä¸åŒåˆ†ç±»å™¨çš„æ¦‚ç‡è¾“å‡ºåˆ†å¸ƒï¼Œå‘ç°ä¸åŒåˆ†ç±»å™¨çš„æ¦‚ç‡è¾“å‡ºå·®åˆ«å¾ˆå¤§ã€‚
å¦‚æœä»¥é€»è¾‘å›å½’ä¸ºçœŸå®æ¦‚ç‡çš„è¿‘ä¼¼çš„è¯ï¼Œ
å…¶ä»–åˆ†ç±»å™¨çš„è¾“å‡ºè¿œè¿œä¸èƒ½è®¤ä¸ºæ˜¯çœŸå®æ¦‚ç‡çš„è¿‘ä¼¼ï¼Œéœ€è¦è¿›ä¸€æ­¥çš„è½¬æ¢ã€‚

> æ¦‚ç‡æ ¡å‡†

> Coding by Scikit-learn

**ä»¥ä¸‹ç»™å‡ºä¸€ç§ç²—ç³™çš„æ¨¡å‹æ¨¡æ‹Ÿ**

```python
import matplotlib.pyplot as plt

# æ•°æ®
from sklearn import datasets
# åˆ†ç±»å™¨
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
# è¯„ä¼°
from sklearn.metrics import (brier_score_loss, precision_score, recall_score, f1_score)
from sklearn.calibration import CalibratedClassifierCV
from sklearn.model_selection import train_test_split

from kivi.ModelSelect import Calibration

calibration = Calibration()

# æ¨¡æ‹Ÿæ•°æ®
X, y = datasets.make_classification(n_samples=100000, n_features=20,
                                    n_informative=2, n_redundant=10,
                                    random_state=42)

# åˆ‡åˆ†æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.99,
                                                    random_state=42)


def plot_calibration_curve(est, name, fig_index):
  """Plot calibration curve for est w/o and with calibration. """
  # ä½¿ç”¨ isotonic calibration è¿›è¡Œæ ¡å‡†
  isotonic = CalibratedClassifierCV(est, cv=2, method='isotonic')

  # ä½¿ç”¨ sigmoid calibration è¿›è¡Œæ ¡å‡†
  sigmoid = CalibratedClassifierCV(est, cv=2, method='sigmoid')

  # é€»è¾‘å›å½’åŸºå‡†
  lr = LogisticRegression(C=1.)

  fig = plt.figure(fig_index, figsize=(10, 10), dpi=150)
  ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)
  ax2 = plt.subplot2grid((3, 1), (2, 0))

  ax1.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")
  for clf, name in [(lr, 'Logistic'),
                    (est, name),
                    (isotonic, name + ' + Isotonic'),
                    (sigmoid, name + ' + Sigmoid')]:
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    if hasattr(clf, "predict_proba"):
      prob_pos = clf.predict_proba(X_test)[:, 1]
    else:
      prob_pos = clf.decision_function(X_test)
      prob_pos =
        (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())

    clf_score = brier_score_loss(y_test, prob_pos, pos_label=y.max())
    print("%s:" % name)
    print("\tBrier: %1.3f" % (clf_score))
    print("\tPrecision: %1.3f" % precision_score(y_test, y_pred))
    print("\tRecall: %1.3f" % recall_score(y_test, y_pred))
    print("\tF1: %1.3f\n" % f1_score(y_test, y_pred))

    fraction_of_positives, mean_predicted_value =
      calibration.calibration_curve(y_test, prob_pos, n_bins=10)

    ax1.plot(mean_predicted_value, fraction_of_positives, "s-",
             label="%s (%1.3f)" % (name, clf_score))

    ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,
             histtype="step", lw=2)

  ax1.set_ylabel("Fraction of positives")
  ax1.set_ylim([-0.05, 1.05])
  ax1.legend(loc="lower right")
  ax1.set_title('Calibration plots  (reliability curve)')

  ax2.set_xlabel("Mean predicted value")
  ax2.set_ylabel("Count")
  ax2.legend(loc="upper center", ncol=2)

  plt.tight_layout()


# Naive Bayes
plot_calibration_curve(GaussianNB(), "Naive Bayes", 1)

# Linear SVC
plot_calibration_curve(LinearSVC(max_iter=10000), "SVC", 2)

plt.show()
```

**Result**

<figure class="half">
    <img src="./img/svc_calibration.png" width="350">
    <img src="./img/naive_bayes_calibration.png" width="350">
</figure>

**ä¸Šå›¾æ˜¾ç¤ºç»è¿‡æ¦‚ç‡æ ¡å‡†ï¼Œæœºå™¨å­¦ä¹ åˆ†ç±»å™¨è¾“å‡ºæ¦‚ç‡è¿‘ä¼¼çœŸå®æ¦‚ç‡ã€‚**

`Naive Bayes`:

| æŒ‡æ ‡      | Logistic | Naive Bayes | Naive Bayes + Isotonic | Naive Bayes + Sigmoid |
|----------|-----------|-------------|-----------------------|-----------------------|
| Brier    | 0.099     | 0.118        | `0.098`              | `0.109`               |
| Precision | 0.872     | 0.857       | 0.883               | 0.861                  |
| Recall    | 0.852     | 0.876       | 0.836               | 0.871                  |
| F1        | 0.862     | 0.867       | 0.859               | 0.866                  |

`SVC`:

| æŒ‡æ ‡      | Logistic | SVC    | SVC + Isotonic | SVC + Sigmoid |
|----------|----------|--------|----------------|---------------|
| Brier    | 0.099    |0.163   |`0.100`         |`0.099`        |
| Precision|0.872     |0.872   |0.853           |0.874          |
| Recall   |0.852     |0.852   |0.878           |0.849          |
| F1       |0.862     |0.862   |0.865           |0.861          |

**ä¸Šè¡¨æ˜¾ç¤ºäº†æ¦‚ç‡æ ¡å‡†å–å¾—äº†è‰¯å¥½çš„æ¦‚ç‡è¾“å‡ºç»“æœ**

### é”™è¯¯åˆ†é…æ¦‚ç‡æ ¡å‡†æ–¹æ³• ğŸ”¨

### æ¬ é‡‡æ ·æ¦‚ç‡æ ¡å‡†æ–¹æ³• ğŸ”¨

> ğŸ“š Reference

- [1] Predicting Good Probabilities with Supervised Learning, A. Niculescu-Mizil & R. Caruana, ICML 2005
- [2] [Reliability diagrams](https://www.cnblogs.com/downtjs/p/3433021.html)
- [3] [ä½¿ç”¨ Isotonic Regression æ ¡å‡†åˆ†ç±»å™¨](https://vividfree.github.io/%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0/2015/12/21/classifier-calibration-with-isotonic-regression)
- [4] Alexandru Niculescu-Mizil, et al. Predicting Good Probabilities With Supervised Learning. ICML2005.
- [5] [ä¿¡ç”¨è¯„åˆ†å¡æ¨¡å‹åˆ†æ•°æ ¡å‡†](https://zhuanlan.zhihu.com/p/82670834)
- [6] Gary King, Langche Zeng. Logistic Regression in Rare Events Data. Political Methodology. 2002
- [7] [Calibrating Probability with Undersampling for Unbalanced Classification](www3.nd.edu/~rjohns15/content/papers/ssci2015_calibrating.pdf)
- [8] ["SAS"-Oversampling](www.data-mining-blog.com/tips-and-tutorials/overrepresentation-oversampling/)
- [9] [é¢å‘ç¨€æœ‰äº‹ä»¶çš„ Logistic Regression æ¨¡å‹æ ¡å‡†](vividfree.github.io/%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0/2015/12/15/model-calibration-for-logistic-regression-in-rare-events-data)


> Editor&Coding: Chensy
