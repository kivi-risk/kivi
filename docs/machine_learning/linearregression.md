## å›å½’åˆ†æ

> å›å½’æ¨¡å‹ç®€ä»‹

|ä¸­æ–‡åç§°    |è‹±æ–‡åç§°    |æè¿°           |ä¼˜åŒ–å…¬å¼          |ä¸€èˆ¬ç”¨é€”|
|-----------|-----------|-------------------|----|----|
|æœ€å°äºŒä¹˜å›å½’|OLS|Ordinary least squares Linear Regression.|||
||Ridge|Linear least squares with l2 regularization.|||
||SGDRegressor|Linear model fitted by minimizing a regularized empirical loss with SGD.|||
||ElasticNet|Linear regression with combined L1 and L2 priors as regularizer.|||
||Lars|Least Angle Regression model a.k.a.|||
||Lasso|Linear Model trained with L1 prior as regularizer (aka the Lasso)|||

> OLSå›å½’ä¸€èˆ¬æµç¨‹

```mermaid
graph LR
    åˆå§‹åˆ†æ-->å˜é‡é€‰æ‹©;
    å˜é‡é€‰æ‹©-->éªŒè¯æ¨¡å‹å‡è®¾;
    éªŒè¯æ¨¡å‹å‡è®¾-->å›å½’è¯Šæ–­;
    å›å½’è¯Šæ–­-->F{æ¨¡å‹æ˜¯å¦æœ‰é—®é¢˜};
    F{æ¨¡å‹æ˜¯å¦æœ‰é—®é¢˜}--æ˜¯-->éªŒè¯æ¨¡å‹å‡è®¾;
    F{æ¨¡å‹æ˜¯å¦æœ‰é—®é¢˜}--å¦-->é¢„æµ‹ä¸è§£é‡Š;
```

### æ™®é€šæœ€å°äºŒä¹˜

**è¿™éƒ¨åˆ†ä¸»è¦æ€»ç»“ä»‹ç»å›å½’å»ºæ¨¡ä¸€èˆ¬æ­¥éª¤ä¸æ–¹æ³•ï¼Œç›¸å…³å›å½’è¯Šæ–­ç†è®ºåˆ†æï¼Œåœ¨ä¸‹é¢çš„ç†è®ºåˆ†æéƒ¨åˆ†ã€‚**

> æ–¹æ³•ä»‹ç»

```python
Class: LinearRegression(
    # çº¿æ€§å›å½’æ–¹æ³•
    # æ•°æ®
    data: [DataFrame] 
    # åœ¨æ•°æ®é›†DataFrameä¸­çš„åç§°
    y_name: [str] y 
    # å¯¹ç»™å®šçš„åˆ—å–å¯¹æ•°ï¼Œå¯ä»¥æ˜¯listç»™å®šåˆ—åç§°ï¼Œ
    # ä¹Ÿå¯ä»¥æ˜¯å­—ç¬¦ä¸²:
    #    'all', å…¨éƒ¨å–å¯¹æ•°
    #    'endog', å› å˜é‡(y)å–å¯¹æ•°
    #    'exdog', è‡ªå˜é‡(X)å–å¯¹æ•°
    log_cols: [str, list] 
    # ä¼°è®¡æ–¹å¼ 'ols' æ™®é€šæœ€å°äºŒä¹˜ or 'rlm' Robust lm
    estimate: [str] 
)
```

> ç¤ºä¾‹ - æ™®é€šæœ€å°äºŒä¹˜å›å½’OLS

```python
# ç¤ºä¾‹æ•°æ®
from kivi import LoadData

load_data = LoadData()
crime_data = load_data.CrimeData()
```

**Result:**

| state      |   violent |   hs_grad |   poverty |   single |   white |   urban |
|:-----------|----------:|----------:|----------:|---------:|--------:|--------:|
| Alabama    |     459.9 |      82.1 |      17.5 |     29   |    70   |   48.65 |
| Alaska     |     632.6 |      91.4 |       9   |     25.5 |    68.3 |   44.46 |
| Arizona    |     423.2 |      84.2 |      16.5 |     25.7 |    80   |   80.07 |
| Arkansas   |     530.3 |      82.4 |      18.8 |     26.3 |    78.4 |   39.54 |
| California |     473.4 |      80.6 |      14.2 |     27.8 |    62.7 |   89.73 |

```python
# è¿›è¡Œçº¿æ€§æ‹Ÿåˆ
from kivi.Model import LinearRegression

lr = LinearRegression(crime_data, y_name='murder')

# æ‰“å°å›å½’ç»“æœ
print(lr.res.summary())
```

**Result:**

```markdown
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                 murder   R-squared:                       0.882
Model:                            OLS   Adj. R-squared:                  0.866
Method:                 Least Squares   F-statistic:                     54.91
Date:                Wed, 19 Feb 2020   Prob (F-statistic):           8.02e-19
Time:                        15:48:38   Log-Likelihood:                -83.303
No. Observations:                  51   AIC:                             180.6
Df Residuals:                      44   BIC:                             194.1
Df Model:                           6                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const        -32.9319     10.523     -3.130      0.003     -54.139     -11.725
violent        0.0077      0.002      4.817      0.000       0.004       0.011
hs_grad        0.2047      0.097      2.106      0.041       0.009       0.401
poverty        0.2989      0.118      2.540      0.015       0.062       0.536
single         0.4358      0.098      4.463      0.000       0.239       0.633
white          0.0238      0.023      1.056      0.297      -0.022       0.069
urban         -0.0018      0.013     -0.139      0.890      -0.028       0.024
==============================================================================
Omnibus:                        1.547   Durbin-Watson:                   2.494
Prob(Omnibus):                  0.461   Jarque-Bera (JB):                0.809
Skew:                           0.251   Prob(JB):                        0.667
Kurtosis:                       3.358   Cond. No.                     2.68e+04
==============================================================================

`Warnings`:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 2.68e+04. This might indicate that there are
strong multicollinearity or other numerical problems.
```

> ç¤ºä¾‹ - OLS æ¨¡å‹å‚æ•°

```python
# æ¨¡å‹æ‹Ÿåˆå‚æ•°
print('Parameters: ', lr.res.params)
>>>
    |         |            0 |
    |:--------|-------------:|
    | const   | -32.9319     |
    | violent |   0.00773022 |
    | hs_grad |   0.204742   |
    | poverty |   0.298897   |
    | single  |   0.435815   |
    | white   |   0.0237816  |
    | urban   |  -0.00178929 |

# R^2
print('R2: ', lr.res.rsquared)
>>> 0.8821788029264949

# æ ‡å‡†å·®
print('Standard errors: ', lr.res.bse)
>>>
    |         |           0 |
    |:--------|------------:|
    | const   | 10.5228     |
    | violent |  0.00160494 |
    | hs_grad |  0.0972317  |
    | poverty |  0.117693   |
    | single  |  0.0976493  |
    | white   |  0.022521   |
    | urban   |  0.0128279  |

# é¢„æµ‹å€¼
print('Predicted values: ', lr.res.predict())
>>> [ 6.87952618  6.01971971  5.47029354  6.91308801  4.92032201  3.04338139
  2.96549525  6.7928318  22.48865498  6.98670125  ...]

...
# å…¶ä»–å‚æ•°è¯·ä½¿ç”¨lr.res."Shift+Tab"æŸ¥çœ‹ã€‚
```

> æ–¹æ³• - å›å½’è¯Šæ–­

```python
# å…±çº¿æ€§: VIF | æ¡ä»¶æ•°
print(lr.vif_value)
print(lr.cond_num)

# æ®‹å·®æ­£æ€æ€§
print(lr.residuals_norm_test)

# å¼‚æ–¹å·®
print(lr.heteroske_test)

# çº¿æ€§æ€§
print(lr.linearity_test)

# å½±å“æµ‹è¯• - é’ˆå¯¹å¼‚å¸¸å€¼
print(lr.influence(lr.res))

# leverage statistics vs. normalized residuals squared - é’ˆå¯¹å¼‚å¸¸å€¼
lr.plot_leverage_resid2(lr.res)
```

> ç¤ºä¾‹ - OLS å›å½’è¯Šæ–­

```python
# æ–¹å·®è†¨èƒ€å› å­ vif
lr.vif_value
>>>{
    'violent': 14.305089857169353,
    'hs_grad': 196.48822505813732,
    'poverty': 45.52237732444601,
    'single': 158.53215210887907,
    'white': 85.56770361098305,
    'urban': 15.612765458337895
}

# æ¡ä»¶æ•°
lr.cond_num
>>> 305.6095363246401

# æ®‹å·®æ­£æ€æ€§æ£€éªŒ
lr.residuals_norm_test
>>>
    {
        'Jarque-Bera': [('Jarque-Bera', 0.8093166627589925),
                        ('Chi^2 two-tail prob.', 0.6672047348403675),
                        ('Skew', 0.25148840048392296),
                        ('Kurtosis', 3.3575880839877508)],
        'Omni': [('Chi^2 score', 1.5467104876147588),
                 ('Two-tail probability', 0.4614621498907301)]
}

# å¼‚æ–¹å·®
lr.heteroske_test
>>>
    {
        'Breush-Pagan': [('Lagrange multiplier statistic', 16.182213547136072),
                          ('p-value', 0.012808659956129832),
                          ('f-value', 3.408302999760919),
                          ('f p-value', 0.0075423318024658795)],
        'Goldfeld-Quandt': [('F statistic', 0.44695800989048784),
                            ('p-value', 0.9531276340571405)]}

# çº¿æ€§æ€§æµ‹è¯•
lr.linearity_test
>>> [('t value', nan), ('p value', nan)]

# å½±å“æµ‹è¯•
lr.influence_test
>>> 
    | state      |   dfb_const |   dfb_violent |   dfb_hs_grad |   dfb_poverty |   dfb_single |    dfb_white |   dfb_urban |     cooks_d |   standard_resid |   hat_diag |   dffits_internal |   student_resid |      dffits |
    |:-----------|------------:|--------------:|--------------:|--------------:|-------------:|-------------:|------------:|------------:|-----------------:|-----------:|------------------:|----------------:|------------:|
    | Alabama    |   0.0227873 |   -0.00779274 |   -0.0248239  |   -0.00841207 |   0.00874497 | -0.00579943  | -0.0229824  | 0.000386988 |        0.172606  |  0.0833467 |        0.0520473  |        0.170691 |  0.0514698  |
    | Alaska     |  -0.457218  |   -0.908514   |    0.23347    |    0.909002   |   0.169231   |  0.354372    |  1.07609    | 0.324969    |       -2.47503   |  0.270789  |       -1.50824    |       -2.6372   | -1.60706    |
    | Arizona    |  -0.001564  |   -0.00109371 |    0.00103174 |    0.00451991 |  -0.00162686 |  0.000679803 |  0.00583248 | 8.43121e-06 |        0.0234327 |  0.0970526 |        0.00768235 |        0.023165 |  0.00759459 |
    | Arkansas   |  -0.065641  |   -0.0747424  |    0.0602794  |   -0.023975   |   0.0668172  |  0.0250836   |  0.0618971  | 0.00431609  |       -0.48784   |  0.11265   |       -0.173818   |       -0.483574 | -0.172298   |
    | California |   0.136569  |    0.00494529 |   -0.134081   |   -0.0603519  |  -0.050556   | -0.0526667   |  0.0242819  | 0.00501404  |        0.397479  |  0.181773  |        0.187345   |        0.393644 |  0.185538   |

# Plot leverage statistics vs. normalized residuals squared
lr.plot_leverage_resid2(lr.res)
```

### RLMå›å½’

> å¼‚å¸¸å€¼å’Œå½±å“æµ‹è¯• - Robust Regression, RLMå›å½’

- åœ¨çº¿æ€§æ€§çš„æ¨¡å‹å‡è®¾ä¹‹ä¸‹ï¼Œæ®‹å·®è¾ƒå¤§çš„è§‚æµ‹å€¼å’Œå¯¹å›å½’ä¼°è®¡å€¼å…·æœ‰è¾ƒå¤§å½±å“çš„è§‚æµ‹å€¼ï¼Œä¼šå¯¹æ¨¡å‹çš„æ‹Ÿåˆé€ æˆè¾ƒå¤§çš„å½±å“ã€‚
ç¨³å¥å›å½’RLMå¯ä»¥å¼‚å¸¸å¥å£®çš„æ–¹å¼è¿›è¡Œå¼‚å¸¸è¯†åˆ«åŠä¼°è®¡ã€‚RLMçš„ä¼˜ç‚¹æ˜¯ï¼Œå³ä½¿æœ‰è®¸å¤šç¦»ç¾¤å€¼ï¼Œä¼°è®¡ç»“æœä¹Ÿä¸ä¼šå—åˆ°å¾ˆå¤§å½±å“ï¼Œ

```python
# åœ¨ LinearRegression æ–¹æ³•ä¸­å¢åŠ å‚æ•° estimate='rlm'
lr = LinearRegression(crime_data, y_name='murder', estimate='rlm')
print(lr.res.summary())
```

**Result: RLMå›å½’**

```markdown
                    Robust linear Model Regression Results                    
==============================================================================
Dep. Variable:                 murder   No. Observations:                   51
Model:                            RLM   Df Residuals:                       44
Method:                          IRLS   Df Model:                            6
Norm:                          HuberT                                         
Scale Est.:                       mad                                         
Cov Type:                          H1                                         
Date:                Wed, 19 Feb 2020                                         
Time:                        17:11:26                                         
No. Iterations:                    50                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const        -33.4435      9.527     -3.510      0.000     -52.117     -14.770
violent        0.0088      0.001      6.046      0.000       0.006       0.012
hs_grad        0.2121      0.088      2.409      0.016       0.040       0.385
poverty        0.2498      0.107      2.344      0.019       0.041       0.459
single         0.4358      0.088      4.929      0.000       0.263       0.609
white          0.0291      0.020      1.428      0.153      -0.011       0.069
urban         -0.0062      0.012     -0.537      0.591      -0.029       0.017
==============================================================================

If the model instance has been used for another fit with different fit
parameters, then the fit options might not be the correct ones anymore .
```

**è¾“å‡ºRLMå›å½’æ‰€ä½¿ç”¨æ ·æœ¬çš„æƒé‡**

```python
# è¾“å‡ºRLMå›å½’æ‰€ä½¿ç”¨æ ·æœ¬çš„æƒé‡
lr.res.weights
>>> 
    |    |        0 |
    |---:|---------:|
    |  0 | 1        |
    |  1 | 0.392772 |
    |  2 | 1        |
    |  3 | 1        |
    |  4 | 1        |
    |  5 | 1        |
    |  6 | 1        |
    |  7 | 0.519129 |
    | ...| ...      |
```

> OLS vs. RLM **æ¯”è¾ƒOLSå›å½’ä¸RLMå›å½’**

```python
import pandas as pd
import numpy as np
from kivi import LoadData
from kivi.Model import LinearRegression

# è½½å…¥æ•°æ®
load_data = LoadData()
crime_data = load_data.CrimeData()

# åˆ¶é€ å¼‚å¸¸å€¼
outlier_point = pd.DataFrame({
    'violent': [1000],
    'murder': [0.5],
})
data = crime_data[['violent', 'murder']].append(outlier_point)

# å›å½’æ‹Ÿåˆ
lr_1 = LinearRegression(data.copy(), y_name='murder', log_cols='all')
lr_2 = LinearRegression(data.copy(), y_name='murder', estimate='rlm', log_cols='all')

# ç»˜å›¾
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(8, 6), dpi=100)
ax.plot(lr_1.exog, lr_1.endog, 'o', label="data")
ax.plot(lr_1.exog, lr_1.res.fittedvalues, 'r--.', label="OLS")
ax.plot(lr_2.exog, lr_2.res.fittedvalues, 'g--.', label="RLM")

ax.scatter(
    np.log(outlier_point.violent),
    np.log(outlier_point.murder),
    marker='*', s=200, c='r',
    label='$outlier\ point$'
)

ax.set_xlabel('y: murder')
ax.set_ylabel('x: violent')
ax.set_title('OLS vs. RLM')
ax.legend()
plt.savefig('olsvsrlm.png', dpi=100)
```

<img src="./img/olsvsrlm.png" width="70%">

**ä¸Šå›¾å¯ä»¥çœ‹åˆ°OLSå›å½’åœ¨åŠ å…¥å¼‚å¸¸ç‚¹åå‡ºç°æ˜æ˜¾åç§»ï¼Œè€ŒRLMå›å½’å¹¶æ— æ˜æ˜¾æ‰°åŠ¨ã€‚**

**Ps: lr æ–¹æ³•ä¸­æä¾›ä¸€ç§ä¾¿æ·ç»˜åˆ¶å›å½’å›¾çš„æ–¹æ³•**

```python
# ä¼ å…¥è‡ªå˜é‡ä¸å›å½’ç»“æœ
lr.plot_regression(lr.exog, lr.res)
plt.savefig('regression.png', dpi=100)
```

<img src="./img/regression.png" width="70%">

### Different variable combinations

**å¯¹å˜é‡è¿›è¡Œç»„åˆï¼Œåˆ†åˆ«è¿›è¡Œ`OLS`å›å½’ï¼Œå¹¶è¿”å›å›å½’ç»“æœã€‚**

> æ–¹æ³•

```python
lr.different_variable_combinations(
    # å˜é‡æ•°ç›®
    features_num: [int],
)

return: (
    # æ¨¡å‹è¯„ä¼°æŒ‡æ ‡
    model_index,
    # å˜é‡è¯„ä¼°æŒ‡æ ‡ 
    variables_index
)
```

> ç¤ºä¾‹

```python
lr = LinearRegression(crime_data, y_name='murder', log_cols='all')
model_index, variables_index = lr.different_variable_combinations(2)
model_index
>>>
|    | columns                        |   nobs |   rsquared |   rsquared_adj |    f_pvalue |     aic |     bic |      DW |
|---:|:-------------------------------|-------:|-----------:|---------------:|------------:|--------:|--------:|--------:|
|  0 | ('log_violent', 'log_hs_grad') |     51 |   0.770324 |       0.760754 | 4.64279e-16 | 29.9655 | 35.7609 | 2.20589 |
|  1 | ('log_violent', 'log_poverty') |     51 |   0.784267 |       0.775278 | 1.03278e-16 | 26.7715 | 32.5669 | 2.23331 |
|  2 | ('log_violent', 'log_single')  |     51 |   0.779263 |       0.770065 | 1.79067e-16 | 27.9409 | 33.7364 | 2.12264 |
|  3 | ('log_violent', 'log_white')   |     51 |   0.678722 |       0.665336 | 1.46265e-12 | 47.0829 | 52.8784 | 2.19153 |
|  4 | ('log_violent', 'log_urban')   |     51 |   0.690411 |       0.677512 | 6.00959e-13 | 45.1928 | 50.9883 | 2.33259 |
|  5 | ('log_hs_grad', 'log_poverty') |     51 |   0.486729 |       0.465342 | 1.1177e-07  | 70.9763 | 76.7718 | 2.26201 |
|  6 | ('log_hs_grad', 'log_single')  |     51 |   0.71805  |       0.706302 | 6.36973e-14 | 40.4235 | 46.2189 | 2.40898 |
|  7 | ('log_hs_grad', 'log_white')   |     51 |   0.522574 |       0.502681 | 1.96671e-08 | 67.2841 | 73.0796 | 2.05115 |
|  8 | ('log_hs_grad', 'log_urban')   |     51 |   0.483264 |       0.461733 | 1.31358e-07 | 71.3195 | 77.115  | 2.18507 |
|  9 | ('log_poverty', 'log_single')  |     51 |   0.751754 |       0.741411 | 3.00026e-15 | 33.9307 | 39.7261 | 2.58784 |
| 10 | ('log_poverty', 'log_white')   |     51 |   0.514543 |       0.494316 | 2.93507e-08 | 68.1349 | 73.9304 | 2.0597  |
| 11 | ('log_poverty', 'log_urban')   |     51 |   0.543552 |       0.524534 | 6.6893e-09  | 64.9925 | 70.7879 | 2.27987 |
| 12 | ('log_single', 'log_white')    |     51 |   0.717324 |       0.705546 | 6.77531e-14 | 40.5546 | 46.3501 | 2.58456 |
| 13 | ('log_single', 'log_urban')    |     51 |   0.683414 |       0.670223 | 1.02751e-12 | 46.3326 | 52.128  | 2.38713 |
| 14 | ('log_white', 'log_urban')     |     51 |   0.204165 |       0.171005 | 0.00416633  | 93.3443 | 99.1398 | 2.09983 |

variables_index
>>>
|    | group_0     |   pvalues_0 |   vif_0 | group_1     |   pvalues_1 |   vif_1 | group_2     |   pvalues_2 |   vif_2 | group_3     |   pvalues_3 |    vif_3 | group_4     |   pvalues_4 |   vif_4 | group_5     |   pvalues_5 |   vif_5 | group_6     |   pvalues_6 |   vif_6 | group_7     |   pvalues_7 |   vif_7 | group_8     |   pvalues_8 |   vif_8 | group_9     |   pvalues_9 |   vif_9 | group_10    |   pvalues_10 |   vif_10 | group_11    |   pvalues_11 |   vif_11 | group_12   |   pvalues_12 |   vif_12 | group_13   |   pvalues_13 |   vif_13 | group_14   |   pvalues_14 |   vif_14 |
|---:|:------------|------------:|--------:|:------------|------------:|--------:|:------------|------------:|--------:|:------------|------------:|---------:|:------------|------------:|--------:|:------------|------------:|--------:|:------------|------------:|--------:|:------------|------------:|--------:|:------------|------------:|--------:|:------------|------------:|--------:|:------------|-------------:|---------:|:------------|-------------:|---------:|:-----------|-------------:|---------:|:-----------|-------------:|---------:|:-----------|-------------:|---------:|
|  0 | const       | 0.00083415  | nan     | const       | 1.72223e-14 | nan     | const       | 3.06931e-13 |  nan    | const       |   0.0116714 | nan      | const       | 4.41181e-09 |  nan    | const       |  0.00695419 | nan     | const       | 0.172344    | nan     | const       | 1.95357e-08 | nan     | const       | 3.68435e-07 | nan     | const       | 8.74493e-14 | nan     | const       |  0.279485    | nan      | const       |  2.4327e-07  | nan      | const      |  4.26216e-07 |  nan     | const      |  2.88531e-11 |  nan     | const      |    0.0424506 |  nan     |
|  1 | log_violent | 1.04028e-10 | 143.707 | log_violent | 1.64279e-12 | 118.617 | log_violent | 3.29432e-05 |  356.32 | log_violent |   2.263e-11 |  81.1268 | log_violent | 9.65614e-13 |  124.77 | log_hs_grad |  0.00326609 | 115.877 | log_hs_grad | 0.0178732   | 284.862 | log_hs_grad | 3.61683e-07 | 416.094 | log_hs_grad | 2.57674e-07 | 100.145 | log_poverty | 0.00063926  | 167.011 | log_poverty |  5.456e-07   |  92.2599 | log_poverty |  1.22742e-08 |  51.8552 | log_single |  1.01239e-12 |  111.371 | log_single |  1.66023e-12 |  121.992 | log_white  |    0.0141707 |   63.418 |
|  2 | log_hs_grad | 5.30742e-05 | 143.707 | log_poverty | 1.1101e-05  | 118.617 | log_single  | 1.96603e-05 |  356.32 | log_white   |   0.534151  |  81.1268 | log_urban   | 0.142841    |  124.77 | log_poverty |  0.0608495  | 115.877 | log_single  | 1.54944e-08 | 284.862 | log_white   | 0.00836446  | 416.094 | log_urban   | 0.0738901   | 100.145 | log_single  | 4.96615e-11 | 167.011 | log_white   |  0.000775054 |  92.2599 | log_urban   |  0.000161909 |  51.8552 | log_white  |  0.0191657   |  111.371 | log_urban  |  0.746989    |  121.992 | log_urban  |    0.205427  |   63.418 |
```


### åˆ†ä½æ•°å›å½’ ğŸ”¨


## ElasticNet å›å½’ ğŸ”¨

$$
\frac{1}{2n_{samples}} * ||y - Xw||^2_2 + \alpha * l1_{ratio} * ||w||_{1} + 0.5 \alpha (1 - l1\_{ratio}) ||w||^2_2
$$

## å›å½’è¯Šæ–­

### åŸºæœ¬è¯Šæ–­å‚æ•°

- çº¿æ€§å›å½’æ‹Ÿåˆä¼˜åº¦: åˆ¤æ–­ç³»æ•°$R^2$
- çº¿æ€§å›å½’çš„æ˜¾è‘—æ€§æ£€éªŒ: å›å½’ç³»æ•°æ£€éªŒ$t-test$ï¼Œçº¿æ€§å…³ç³»æ£€éªŒ$F-test$

---

### å¤šé‡å…±çº¿æ€§

#### å…±çº¿æ€§ç†è®ºåˆ†æ

> å¤šé‡å…±çº¿æ€§æ˜¯ä½¿ç”¨çº¿æ€§å›å½’ç®—æ³•æ—¶ç»å¸¸è¦é¢å¯¹çš„ä¸€ä¸ªé—®é¢˜ã€‚
åœ¨å…¶ä»–ç®—æ³•ä¸­ï¼Œä¾‹å¦‚å†³ç­–æ ‘å’Œè´å¶æ–¯ï¼Œå‰è€…çš„å»ºæ¨¡è¿‡ç¨‹æ˜¯é€æ­¥é€’è¿›ï¼Œ
æ¯æ¬¡æ‹†åˆ†åªæœ‰ä¸€ä¸ªå˜é‡å‚ä¸ï¼Œè¿™ç§å»ºæ¨¡æœºåˆ¶å«æœ‰æŠ—å¤šé‡å…±çº¿æ€§å¹²æ‰°çš„åŠŸèƒ½ï¼›
åè€…å¹²è„†å‡å®šå˜é‡ä¹‹é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œå› æ­¤ä»è¡¨é¢ä¸Šçœ‹ï¼Œä¹Ÿæ²¡æœ‰å¤šé‡å…±çº¿æ€§çš„é—®é¢˜ã€‚
ä½†æ˜¯å¯¹äºå›å½’ç®—æ³•ï¼Œä¸è®ºæ˜¯ä¸€èˆ¬å›å½’ï¼Œé€»è¾‘å›å½’ï¼Œæˆ–å­˜æ´»åˆ†æï¼Œ
éƒ½è¦åŒæ—¶è€ƒè™‘å¤šä¸ªé¢„æµ‹å› å­ï¼Œå› æ­¤å¤šé‡å…±çº¿æ€§æ˜¯ä¸å¯é¿å…éœ€è¦é¢å¯¹çš„ï¼Œ
åœ¨å¾ˆå¤šæ—¶å€™ï¼Œå¤šé‡å…±çº¿æ€§æ˜¯ä¸€ä¸ªæ™®éçš„ç°è±¡ã€‚
åœ¨æ„é€ é¢„æµ‹æ¨¡å‹æ—¶å¦‚ä½•å¤„ç†å¤šé‡å…±çº¿æ€§æ˜¯ä¸€ä¸ªæ¯”è¾ƒå¾®å¦™çš„è®®é¢˜ã€‚
æ—¢ä¸èƒ½ä¸åŠ æ§åˆ¶ï¼Œåˆä¸èƒ½ä¸€åˆ€åˆ‡ï¼Œè®¤ä¸ºå‡¡æ˜¯å¤šé‡å…±çº¿æ€§å°±åº”è¯¥æ¶ˆé™¤ã€‚

å‡è®¾æœ‰$k$ä¸ªè‡ªå˜é‡çš„å¤šå…ƒçº¿æ€§å›å½’æ¨¡å‹: 
$$
\begin{aligned}
y & = \theta_0 + \theta_1 x_1 + ... + \theta_k x_k +\varepsilon \\\\
& = X\theta + \varepsilon \\\\
\varepsilon & = N(0, \sigma^2) \\\\
\hat{\theta} & = (X^T X)^{-1}X^Ty 
\end{aligned}
$$
è¯¥æ±‚è§£å…¬å¼å”¯ä¸€çš„æ¡ä»¶æ˜¯çŸ©é˜µ$X$æ˜¯åˆ—æ»¡ç§©çš„ï¼Œä¸ç„¶ä¼šæœ‰æ— ç©·å¤šè§£ï¼Œ
å½“å„å˜é‡ä¹‹é—´å­˜åœ¨å…±çº¿æ€§é—®é¢˜ï¼Œå³å„å˜é‡ä¹‹é—´å­˜åœ¨éƒ¨åˆ†çº¿æ€§ç›¸å…³æ—¶ï¼Œä¾‹å¦‚ï¼š
$$x_3 = x_2 + x_1 + \varepsilon$$
æ˜“çŸ¥æ­¤æ—¶$X$è¿‘ä¹æ˜¯ä¸æ»¡ç§©çš„ï¼Œ$X^TX$è¿‘ä¹æ˜¯å¥‡å¼‚çš„ï¼Œ$X$çš„æœ€å°å¥‡å¼‚å€¼ä¼šéå¸¸å°ã€‚

---

> **æ‰°åŠ¨åˆ†æ**: å¯¹äºä¸€ä¸ªæ–¹ç¨‹æˆ–è€…ç³»ç»Ÿè€Œè¨€ï¼Œå½“è¾“å…¥æœ‰ä¸€ä¸ªéå¸¸å¾®å°çš„æ‰°åŠ¨æ—¶ï¼Œ
å¸Œæœ›æ–¹ç¨‹æˆ–ç³»ç»Ÿçš„è¾“å‡ºå˜åŒ–ä¹Ÿéå¸¸å¾®å°ï¼Œå¦‚æœè¾“å‡ºçš„å˜åŒ–éå¸¸å¤§ï¼Œä¸”ä¸èƒ½è¢«æ§åˆ¶ï¼Œ
é‚£è¿™ä¸ªç³»ç»Ÿçš„é¢„æµ‹å°±æ— æ•ˆäº†ã€‚åœ¨çŸ©é˜µè®¡ç®—ä¸­ï¼Œè¿™å«åšæ‰°åŠ¨åˆ†æã€‚

**ã€æ‰°åŠ¨åˆ†æå®šç†ã€‘** 
è®¾éå¥‡å¼‚æ–¹é˜µ$A$æ»¡è¶³æ–¹ç¨‹
$$
Ax = y
$$
å®ƒçš„ç²¾ç¡®è§£ä¸º $x^\*$ï¼Œå½“$A$å­˜åœ¨ä¸€ä¸ªå°æ‰°åŠ¨æ—¶ï¼Œå‡è®¾$\hat{x}$æ˜¯æ–°æ–¹ç¨‹çš„è§£ï¼š
$$
(A+\delta A)\hat{x} = y
$$
å¯ä»¥è¯æ˜$x^*$çš„æ‰°åŠ¨æ»¡è¶³ï¼š
$$
\frac{||\delta x||}{||\hat{x}||} \le k(A) \frac{||\delta A||}{||A||}
$$
å…¶ä¸­$k(A) = ||A^{-1}|| \cdot ||A||$
æ˜¯éå¥‡å¼‚æ–¹é˜µçš„æ¡ä»¶æ•°ï¼Œä¸”æ­¤æ—¶çŸ©é˜µèŒƒæ•°ç­‰ä»·äºçŸ©é˜µæœ€å¤§çš„å¥‡å¼‚å€¼ï¼Œ
å³çŸ©é˜µçš„æ¡ä»¶æ•°ç­‰ä»·äº`æœ€å¤§å¥‡å¼‚å€¼/æœ€å°å¥‡å¼‚å€¼`ã€‚

å¯ä»¥çœ‹åˆ°çŸ©é˜µçš„æ¡ä»¶æ•°è¶Šå¤§ï¼Œæ‰°åŠ¨å°±è¶Šå¤§ï¼Œå³$x$çš„æ±‚è§£å€¼ä¼šå˜å¾—éå¸¸ä¸å‡†ç¡®ã€‚
å›åˆ°ä¸Šé¢è®²çš„çº¿æ€§å›å½’é—®é¢˜ï¼Œå®¹æ˜“è¯æ˜æœ€å°äºŒä¹˜æ³•çš„è§£æ»¡è¶³ä¸‹é¢çš„æ­£å®šæ–¹ç¨‹ï¼š
$$
X^TX\hat{\theta} = X^T y
$$

æ­¤æ—¶

$$
k(X^TX) = \frac{\lambda_max(X^T X)}{\lambda_max(X^T X)} = \frac{\sigma_{max}^2(X)}{\sigma_{min}^2(X)}
$$

å½“æ–¹ç¨‹æœ‰å…±çº¿æ€§é—®é¢˜æ—¶ï¼Œ$X$çš„æœ€å°ç‰¹å¾å€¼éå¸¸å°ï¼Œç›¸åº”çš„ï¼Œ
ä¸Šè¿°çš„æ¡ä»¶æ•°ä¼šéå¸¸å¤§ã€‚ä¹Ÿå°±æ˜¯è¯´æœºå™¨å­¦ä¹ ä¸­çš„å…±çº¿æ€§é—®é¢˜å®é™…ä¸Šå°±æ˜¯çŸ©é˜µè®¡ç®—ä¸­çš„æ¡ä»¶æ•°é—®é¢˜ã€‚

ä»å®é™…åº”ç”¨çš„è§’åº¦:
- ä¸€èˆ¬è‹¥`K<100`ï¼Œåˆ™è®¤ä¸ºå¤šé‡å…±çº¿æ€§çš„ç¨‹åº¦å¾ˆå°
- `100<=K<=1000`ï¼Œåˆ™è®¤ä¸ºå­˜åœ¨ä¸€èˆ¬ç¨‹åº¦ä¸Šçš„å¤šé‡å…±çº¿æ€§
- `K>1000`ï¼Œåˆ™å°±è®¤ä¸ºå­˜åœ¨ä¸¥é‡çš„å¤šé‡å…±çº¿æ€§

> æ–¹å·®åˆ†æ

ä»ç»Ÿè®¡å­¦çš„è§’åº¦æ¥çœ‹å…±çº¿æ€§ã€‚å¯ä»¥è¯æ˜å‚æ•°$\theta$çš„åæ–¹å·®çŸ©é˜µä¸º

$$
Var(\hat{\theta}) = Var(\hat{\theta} - \theta) = Var[(X^TX)^{-1}X^T \varepsilon]
$$

åˆå¯¹ä»»æ„çš„å¸¸æ•°çŸ©é˜µAå’Œéšæœºå˜é‡xæœ‰

$$
Var(Ax) = A \cdot Var(x) \cdot A^T
$$

ä»£å…¥ä¸Šå¼å³å¯å¾—

$$
Var(\hat{\theta}) = \sigma^2(X^TX)^{-1}
$$

å…·ä½“åˆ°æ¯ä¸ªå‚æ•°ï¼Œæœ‰ï¼š

$$
Var(\hat{\theta}_i) = \frac{\sigma^2}{(n-1)Var(x_j)} \cdot \frac{1}{1-R_i^2}
$$

å…¶ä¸­$Ri2$æ˜¯å°†ç¬¬iä¸ªå˜é‡$x_i$ä½œä¸ºå› å˜é‡ï¼Œå…¶ä»–$k-1$ä¸ªå˜é‡ä½œä¸ºè‡ªå˜é‡è¿›è¡Œçº¿æ€§å›å½’è·å¾—çš„$R2$ï¼Œä¸”ä»¤

$$
VIF_i = \frac{1}{1-R_i^2}
$$

ä¸ºæ–¹å·®è†¨èƒ€å› å­(variance inflation factorï¼ŒVIF)ã€‚å½“$R_i^2 \sim 1$æ—¶ï¼Œ
å³å½“ç¬¬$i$ä¸ªå˜é‡å’Œå…¶ä»–å˜é‡ä¹‹é—´å­˜åœ¨çº¿æ€§å…³ç³»æ—¶ï¼Œ`VIF`è¶‹äºæ— ç©·å¤§ã€‚
æ‰€ä»¥`VIF`çš„å¤§å°ååº”äº†å˜é‡çš„å…±çº¿æ€§ç¨‹åº¦ã€‚ä¸€èˆ¬åœ°ï¼Œå½“`VIF`å¤§äº`5`æˆ–`10`æ—¶ï¼Œ
è®¤ä¸ºæ¨¡å‹å­˜åœ¨ä¸¥é‡çš„å…±çº¿æ€§é—®é¢˜ã€‚

> å…±çº¿æ€§çš„å½±å“

åŒæ—¶è€ƒè™‘å‚æ•°æ˜¾è‘—æ€§æ£€éªŒçš„`t`ç»Ÿè®¡é‡ï¼š

$$
t = \frac{\hat{\theta}_i}{std(\hat{\theta}_i)} \sim t(n-k-1)
$$

å½“å­˜åœ¨å…±çº¿æ€§æ—¶ï¼Œå‚æ•°çš„æ ‡å‡†å·®åå¤§ï¼Œç›¸åº”çš„`t ç»Ÿè®¡é‡`ä¼šåå°ï¼Œ
è¿™æ ·å®¹æ˜“æ·˜æ±°ä¸€äº›ä¸åº”æ·˜æ±°çš„è§£é‡Šå˜é‡ï¼Œä½¿ç»Ÿè®¡æ£€éªŒçš„ç»“æœå¤±å»å¯é æ€§ã€‚

å¦å¤–è€ƒè™‘çº¿æ€§å›å½’çš„æ®‹å·®

$$
\hat{\varepsilon} = y - X\hat{\theta} = M\varepsilon
$$

å…¶ä¸­$M$æ˜¯ä¸€ä¸ªæŠ•å½±çŸ©é˜µï¼Œä¸”æ»¡è¶³

$$
M = I - X(X^TX)^{-1}X^T
$$

æ˜“è¯æ˜

$$
||\hat{\varepsilon}||^2_2 = \varepsilon^T M \varepsilon \le ||M||_F^2 \cdot ||\varepsilon||_2^2 = (n-k) ||\varepsilon||^2_2
$$

è€ŒçŸ©é˜µ$M$çš„èŒƒæ•°ä¸$X$çš„æ¡ä»¶æ•°æ¯«æ— å…³ç³»ï¼Œ
äºæ˜¯å¯ä»¥å¾—å‡ºå…±çº¿æ€§å¹¶ä¸å½±å“æ¨¡å‹çš„è®­ç»ƒç²¾åº¦ã€‚
ä½†æ˜¯å¯¹äºæ³›åŒ–ç²¾åº¦ï¼Œç”±äºå‚æ•°çš„ä¼°è®¡å·²ç»ä¸å‡†ç¡®å•¦ï¼Œ
æ‰€ä»¥æ³›åŒ–è¯¯å·®è‚¯å®šè¦å·®äº›ï¼Œå…·ä½“å·®å¤šå°‘ï¼Œå¾ˆéš¾ç”¨å…¬å¼è¡¨ç¤ºå‡ºæ¥ã€‚

å…±çº¿æ€§é—®é¢˜å¯¹çº¿æ€§å›å½’æ¨¡å‹æœ‰å¦‚ä¸‹å½±å“ï¼š
- å‚æ•°çš„æ–¹å·®å¢å¤§ï¼›
- éš¾ä»¥åŒºåˆ†æ¯ä¸ªè§£é‡Šå˜é‡çš„å•ç‹¬å½±å“ï¼›
- å˜é‡çš„æ˜¾è‘—æ€§æ£€éªŒå¤±å»æ„ä¹‰ï¼›
- å›å½’æ¨¡å‹ç¼ºä¹ç¨³å®šæ€§ã€‚æ ·æœ¬çš„å¾®å°æ‰°åŠ¨éƒ½å¯èƒ½å¸¦æ¥å‚æ•°å¾ˆå¤§çš„å˜åŒ–ï¼›
- å½±å“æ¨¡å‹çš„æ³›åŒ–è¯¯å·®ã€‚

#### å…±çº¿æ€§æ£€æµ‹æ–¹æ³•

- VIF

```python
lr.vif_value
```

- æ¡ä»¶æ•°`condition number`

```python
lr.cond_num
```

- ç›¸å…³æ€§åˆ†æ: æ£€éªŒå˜é‡ä¹‹é—´çš„ç›¸å…³ç³»æ•°ï¼Œå‚è€ƒ: [ç›¸å…³ç³»æ•°](./descriptive_statistics/descriptive_statistics?id=ç›¸å…³ç³»æ•°ç¤ºä¾‹)


#### å…±çº¿æ€§è§£å†³æ–¹æ³•

- ä»æ•°æ®çš„è§’åº¦
    - æå‰ç­›é€‰å˜é‡
    - æœ€ä¼˜å­é›†æ³•
- ä»æ¨¡å‹è§’åº¦è§„é¿
    - PCA é™ç»´æ³•
    - é€æ­¥å›å½’
    - å²­å›å½’ã€L2æ­£åˆ™åŒ–
    - LASSO å›å½’
    - ElasticNet å›å½’
    - åæœ€å°äºŒä¹˜ PLS

---

### å¼‚æ–¹å·®

å¼‚æ–¹å·®æ€§æ˜¯ç›¸å¯¹äºåŒæ–¹å·®è€Œè¨€çš„ã€‚æ‰€è°“åŒæ–¹å·®ï¼Œæ˜¯ä¸ºäº†ä¿è¯å›å½’å‚æ•°ä¼°è®¡é‡å…·æœ‰è‰¯å¥½çš„ç»Ÿè®¡æ€§è´¨ï¼Œ
ç»å…¸çº¿æ€§å›å½’æ¨¡å‹çš„ä¸€ä¸ªé‡è¦å‡å®šï¼šæ€»ä½“å›å½’å‡½æ•°ä¸­çš„éšæœºè¯¯å·®é¡¹æ»¡è¶³åŒæ–¹å·®æ€§ï¼Œ
å³å®ƒä»¬éƒ½æœ‰ç›¸åŒçš„æ–¹å·®ã€‚

> æ¥æº

1. æ¨¡å‹ä¸­ç¼ºå°‘æŸäº›è§£é‡Šå˜é‡ï¼Œä»è€Œéšæœºæ‰°åŠ¨é¡¹äº§ç”Ÿç³»ç»Ÿæ¨¡å¼
1. æµ‹é‡è¯¯å·®
1. æ¨¡å‹å‡½æ•°å½¢å¼è®¾ç½®ä¸æ­£ç¡®
1. å¼‚å¸¸å€¼çš„å‡ºç°

> å¼‚æ–¹å·® - æ£€éªŒæ–¹å¼

```python
lr.heteroske_test
>>>
    {
        'Breush-Pagan': [('Lagrange multiplier statistic', 16.182213547136072),
                          ('p-value', 0.012808659956129832),
                          ('f-value', 3.408302999760919),
                          ('f p-value', 0.0075423318024658795)],
        'Goldfeld-Quandt': [('F statistic', 0.44695800989048784),
                            ('p-value', 0.9531276340571405)]}
```

> åæœ:

1. å‚æ•°ä¼°è®¡é‡ä»ç„¶æ˜¯çº¿æ€§æ— åçš„ï¼Œä½†ä¸æ˜¯æœ‰æ•ˆçš„ï¼›
2. å¼‚æ–¹å·®æ¨¡å‹ä¸­çš„æ–¹å·®ä¸å†å…·æœ‰æœ€å°æ–¹å·®æ€§ï¼›
3. `t`æ£€éªŒå¤±å»ä½œç”¨ï¼›
4. æ¨¡å‹çš„é¢„æµ‹ä½œç”¨é­åˆ°ç ´åã€‚

> è¡¥æ•‘æªæ–½ï¼š

1. å¯¹æ¨¡å‹å˜æ¢ï¼Œå½“å¯ä»¥ç¡®å®šå¼‚æ–¹å·®çš„å…·ä½“å½¢å¼æ—¶ï¼Œå°†æ¨¡å‹ä½œé€‚å½“å˜æ¢æœ‰å¯èƒ½æ¶ˆé™¤æˆ–å‡è½»å¼‚æ–¹å·®çš„å½±å“ã€‚
1. åŠ æƒæœ€å°äºŒä¹˜æ³•ï¼Œå¯¹åŸæ¨¡å‹å˜æ¢çš„æ–¹æ³•ä¸åŠ æƒäºŒä¹˜æ³•å®é™…ä¸Šæ˜¯ç­‰ä»·çš„ï¼Œå¯ä»¥æ¶ˆé™¤å¼‚æ–¹å·®ã€‚
1. å¯¹æ•°å˜æ¢ï¼Œè¿ç”¨å¯¹æ•°å˜æ¢èƒ½ä½¿æµ‹å®šå˜é‡å€¼çš„å°ºåº¦ç¼©å°ã€‚
å®ƒå¯ä»¥å°†ä¸¤ä¸ªæ•°å€¼ä¹‹é—´åŸæ¥`10`å€çš„å·®å¼‚ç¼©å°åˆ°åªæœ‰`2`å€çš„å·®å¼‚ã€‚
å…¶æ¬¡ï¼Œç»è¿‡å¯¹æ•°å˜æ¢åçš„çº¿æ€§æ¨¡å‹ï¼Œå…¶æ®‹å·® $\varepsilon$ è¡¨ç¤ºç›¸å¯¹è¯¯å·®ï¼Œ
è€Œç›¸å¯¹è¯¯å·®å¾€å¾€æ¯”ç»å¯¹è¯¯å·®æœ‰è¾ƒå°çš„å·®å¼‚ã€‚

**æ›´å¤šæ–¹æ³•è¯·å‚è€ƒ: æ®‹å·®åˆ†æ - æ–¹å·®é½æ€§æ£€éªŒ**

---

### æ®‹å·®åˆ†æ

**çº¿æ€§å›å½’å‡è®¾: æ®‹å·® $\varepsilon \sim N(0, \sigma^2)$ ç‹¬ç«‹åŒåˆ†å¸ƒã€‚**

1. æ­£æ€æ€§æ£€éªŒ
1. ç‹¬ç«‹æ€§æ£€éªŒ
1. æ–¹å·®é½æ€§æ£€éªŒ

åªæœ‰ä»¥ä¸Šå‡è®¾è¢«éªŒè¯ï¼Œæ¨¡å‹æ‰æ˜¯æˆç«‹çš„ã€‚

#### æ­£æ€æ€§æ£€éªŒ

> ç›´æ–¹å›¾ç›®æµ‹: æ ¹æ®æ•°æ®åˆ†å¸ƒçš„ç›´æ–¹å›¾ä¸æ ‡å‡†æ­£æ€åˆ†å¸ƒå¯¹æ¯”è¿›è¡Œæ£€éªŒï¼Œä¸»è¦æ˜¯é€šè¿‡ç›®æµ‹ã€‚

```python
lr.plot_resid()
plt.savefig('resid.png', dpi=100)
```

<img src="./img/resid.png" width="50%">

> PP or QQ å›¾

PPå›¾æ˜¯å¯¹æ¯”æ­£æ€åˆ†å¸ƒçš„ç´¯ç§¯æ¦‚ç‡å€¼å’Œå®é™…åˆ†å¸ƒçš„ç´¯ç§¯æ¦‚ç‡å€¼ã€‚

```python
lr.plot_pp()
plt.savefig('pp.png', dpi=100)
```

<img src="./img/pp.png" width="50%">

`QQ`å›¾æ˜¯é€šè¿‡æŠŠæµ‹è¯•æ ·æœ¬æ•°æ®çš„åˆ†ä½æ•°ä¸å·²çŸ¥åˆ†å¸ƒç›¸æ¯”è¾ƒï¼Œ
ä»è€Œæ¥æ£€éªŒæ•°æ®çš„åˆ†å¸ƒæƒ…å†µã€‚å¯¹åº”äºæ­£æ€åˆ†å¸ƒçš„QQå›¾ï¼Œ
å°±æ˜¯ç”±æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„åˆ†ä½æ•°ä¸ºæ¨ªåæ ‡ï¼Œæ ·æœ¬å€¼ä¸ºçºµåæ ‡çš„æ•£ç‚¹å›¾ã€‚

```python
lr.plot_qq()
plt.savefig('qq.png', dpi=100)
```

<img src="./img/qq.png" width="50%">

ppå›¾å’Œqqå›¾åˆ¤æ–­æ ‡å‡†æ˜¯ï¼š
å¦‚æœè§‚å¯Ÿç‚¹éƒ½æ¯”è¾ƒå‡åŒ€çš„åˆ†å¸ƒåœ¨ç›´çº¿é™„è¿‘ï¼Œ
å°±å¯ä»¥è¯´æ˜å˜é‡è¿‘ä¼¼çš„æœä»æ­£æ€åˆ†å¸ƒï¼Œå¦åˆ™ä¸æœä»æ­£æ€åˆ†å¸ƒã€‚

> Shapiroæ£€éªŒ

å‡è®¾å˜é‡æ˜¯æœä»æ­£æ€åˆ†å¸ƒçš„ï¼Œç„¶åå¯¹å‡è®¾è¿›è¡Œæ£€éªŒã€‚
ä¸€èˆ¬åœ°æ•°æ®é‡ä½äº5000åˆ™å¯ä»¥ä½¿ç”¨Shapiroæ£€éªŒï¼Œ
å¤§äº5000çš„æ•°æ®é‡å¯ä»¥ä½¿ç”¨K-Sæ£€éªŒï¼Œ
è¿™ç§æ–¹æ³•åœ¨scipyåº“ä¸­å¯ä»¥ç›´æ¥è°ƒç”¨ï¼š

```python
import scipy.stats as stats
stats.shapiro(residual)
```

å¦‚æœ`p`å€¼éå¸¸å°ï¼Œè¿œè¿œå°äº`0.05`ï¼Œåˆ™æ‹’ç»åŸå‡è®¾ï¼Œè¯´æ˜æ®‹å·®ä¸æœä»æ­£æ€åˆ†å¸ƒã€‚


> Jarque-Beraæ£€éªŒ

Jarque-Beraæ£€éªŒåŸºäºæ•°æ®æ ·æœ¬çš„ååº¦å’Œå³°åº¦ï¼Œè¯„ä»·ç»™å®šæ•°æ®æœä»æœªçŸ¥å‡å€¼å’Œæ–¹å·®æ­£æ€åˆ†å¸ƒçš„å‡è®¾æ˜¯å¦æˆç«‹ã€‚

```python
# æ®‹å·®æ­£æ€æ€§æ£€éªŒ
lr.residuals_norm_test
>>>
    {
        'Jarque-Bera': [('Jarque-Bera', 0.8093166627589925),
                        ('Chi^2 two-tail prob.', 0.6672047348403675),
                        ('Skew', 0.25148840048392296),
                        ('Kurtosis', 3.3575880839877508)],
        'Omni': [('Chi^2 score', 1.5467104876147588),
                 ('Two-tail probability', 0.4614621498907301)]
}
```

#### ç‹¬ç«‹æ€§æ£€éªŒ

æ®‹å·®çš„ç‹¬ç«‹æ€§å¯ä»¥é€šè¿‡`Durbin-Watson`ç»Ÿè®¡é‡`DW`æ¥æ£€éªŒã€‚

- åŸå‡è®¾: $p=0$ï¼Œå³å‰åæ‰°åŠ¨é¡¹ä¸å­˜åœ¨ç›¸å…³æ€§
- èƒŒè´£å‡è®¾: $p \ne 0$ï¼Œå³è¿‘é‚»çš„å‰åæ‰°åŠ¨é¡¹å­˜åœ¨ç›¸å…³æ€§

åˆ¤æ–­æ ‡å‡†æ˜¯ï¼š

- $p=0ï¼ŒDW=2$ï¼šæ‰°åŠ¨é¡¹å®Œå…¨ä¸ç›¸å…³
- $p=1ï¼ŒDW=0$ï¼šæ‰°åŠ¨é¡¹å®Œå…¨æ­£ç›¸å…³
- $p=-1ï¼ŒDW=4$ï¼šæ‰°åŠ¨é¡¹å®Œå…¨è´Ÿç›¸å…³

**ç¤ºä¾‹**

```python
lr.DW
>>> 2.4939455362349867
```
DWå€¼ä¸º2.5ï¼Œè¯´æ˜æ®‹å·®ä¹‹é—´æ˜¯è¿‘ä¼¼ä¸ç›¸å…³çš„ï¼ŒåŸºæœ¬æ»¡è¶³ç‹¬ç«‹æ€§å‡è®¾ã€‚

#### æ–¹å·®é½æ€§æ£€éªŒ

å¦‚æœæ®‹å·®éšç€è‡ªå˜é‡å¢å‘ç”Ÿéšæœºå˜åŒ–ï¼Œä¸Šä¸‹ç•ŒåŸºæœ¬å¯¹ç§°ï¼Œ
æ— æ˜æ˜¾è‡ªç›¸å…³ï¼Œæ–¹å·®ä¸ºé½æ€§ï¼Œæˆ‘ä»¬å°±è¯´è¿™æ˜¯æ­£å¸¸çš„æ®‹å·®ã€‚
åˆ¤æ–­æ–¹å·®é½æ€§æ£€éªŒçš„æ–¹æ³•ä¸€èˆ¬æœ‰ä¸¤ä¸ªï¼š

- å›¾å½¢æ³•
- BPæ£€éªŒ

**å›¾å½¢æ³•**
å›¾å½¢æ³•å°±æ˜¯ç”»å‡ºè‡ªå˜é‡ä¸æ®‹å·®çš„æ•£ç‚¹å›¾ï¼Œè‡ªå˜é‡ä¸ºæ¨ªåæ ‡ï¼Œæ®‹å·®ä¸ºçºµåæ ‡ã€‚

```python
lr.plot_variance_homogeneity(lr.exog, lr.res.resid)
```

**Result**

<img src="./img/variance_homogeneity.png" width="50%">

å›¾å½¢æ³•å¯ä»¥çœ‹å‡ºï¼šæ®‹å·®çš„æ–¹å·®ï¼ˆå³è§‚å¯Ÿç‚¹ç›¸å¯¹çº¢è‰²è™šçº¿çš„ä¸Šä¸‹æµ®åŠ¨å¤§å°ï¼‰
éšç€è‡ªå˜é‡å˜åŒ–æœ‰å¾ˆå¤§çš„æµ®åŠ¨ï¼Œè¯´æ˜äº†æ®‹å·®çš„æ–¹å·®æ˜¯éé½æ€§çš„ã€‚

å¦‚æœæ®‹å·®æ–¹å·®ä¸æ˜¯é½æ€§çš„ï¼Œæœ‰å¾ˆå¤šä¿®æ­£çš„æ–¹æ³•ï¼Œæ¯”å¦‚åŠ æƒæœ€å°äºŒä¹˜æ³•ï¼Œç¨³å¥å›å½’ç­‰ï¼Œ
è€Œæœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯å¯¹`å˜é‡å–è‡ªç„¶å¯¹æ•°`ã€‚è€Œå–å¯¹æ•°ä»ä¸šåŠ¡ä¸Šæ¥è¯´ä¹Ÿæ˜¯æœ‰æ„ä¹‰çš„ï¼Œ
è§£é‡Šå˜é‡å’Œè¢«è§£é‡Šå˜é‡çš„è¡¨è¾¾å½¢å¼ä¸åŒï¼Œå¯¹å›å½’ç³»æ•°çš„è§£é‡Šä¹Ÿä¸åŒã€‚
ä¸‹é¢æ˜¯ä¸åŒè½¬æ¢æƒ…å†µä¸‹çš„è§£é‡Šï¼š

|æ¨¡å‹          |å› å˜é‡   |è‡ªå˜é‡    |å¯¹$\beta_1$çš„è§£é‡Š|
|--------------|--------|---------|----------------|
|æ°´å¹³å€¼ - æ°´å¹³å€¼ |$y$     |$x$      |$\Delta y = \beta_i \Delta x$|
|å¯¹æ•° - æ°´å¹³å€¼   |$y$     |$log(x)$ |$\Delta y = (\beta_i / 100) \\% \Delta x$|
|æ°´å¹³å€¼ - å¯¹æ•°   |$log(y)$|$x$      |$\\% \Delta y (100 \beta_i) \Delta x$|
|å¯¹æ•° - å¯¹æ•°     |$log(y)$|$log(x)$ |$\\% \Delta y = \beta_i \\% \Delta x$|

**æ‹Ÿåˆæ•°æ®å¯¹æ•°åŒ–**

```python
# æ·»åŠ å‚æ•° log_cols: [str, list]ï¼Œå¯¹ç»™å®šçš„åˆ—å–å¯¹æ•°ï¼Œå¯ä»¥æ˜¯listç»™å®šåˆ—åç§°ï¼Œ
#    ä¹Ÿå¯ä»¥æ˜¯å­—ç¬¦ä¸²:
#        'all', å…¨éƒ¨å–å¯¹æ•°
#        'endog', å› å˜é‡(y)å–å¯¹æ•°
#        'exdog', è‡ªå˜é‡(X)å–å¯¹æ•°
LinearRegression(crime_data, y_name='murder', log_cols='all')
```

å¯¹æ•°è½¬æ¢åçš„æ•ˆæœå¯ä»¥é€šè¿‡$R^2$æˆ–è€…$ä¿®æ­£R^2$çš„ç»“æœæ¯”å¯¹å¾—å‡ºï¼Œ
å¦‚æœæ–¹å·®é€šè¿‡å–å¯¹æ•°å˜æ¢å˜æˆé½æ€§ï¼Œé‚£ä¹ˆå®ƒçš„$R^2$åº”è¯¥æ¯”å˜æ¢ä¹‹å‰æ•°å€¼é«˜ï¼Œ
å³ä¼šå–å¾—æ›´å¥½çš„æ•ˆæœã€‚

**BPæ£€éªŒ**

```python
lr.bp_test()

>>> 
    (0.16586685109032384,
     0.6838114989412791,
     0.1643444790856123,
     0.6856254489662914)
```

- ä¸Šè¿°å‚æ•°ï¼š
    - ç¬¬ä¸€ä¸ªä¸ºï¼šLMç»Ÿè®¡é‡å€¼
    - ç¬¬äºŒä¸ªä¸ºï¼šå“åº”çš„på€¼ï¼Œ0.68è¿œå¤§äºæ˜¾è‘—æ€§æ°´å¹³0.05ï¼Œå› æ­¤æ¥å—åŸå‡è®¾ï¼Œå³æ®‹å·®æ–¹å·®æ˜¯ä¸€ä¸ªå¸¸æ•°
    - ç¬¬ä¸‰ä¸ªä¸ºï¼šFç»Ÿè®¡é‡å€¼ï¼Œç”¨æ¥æ£€éªŒæ®‹å·®å¹³æ–¹é¡¹ä¸è‡ªå˜é‡ä¹‹é—´æ˜¯å¦ç‹¬ç«‹ï¼Œå¦‚æœç‹¬ç«‹åˆ™è¡¨æ˜æ®‹å·®æ–¹å·®é½æ€§
    - ç¬¬å››ä¸ªä¸ºï¼šFç»Ÿè®¡é‡å¯¹åº”çš„på€¼ï¼Œä¹Ÿæ˜¯è¿œå¤§äº0.05çš„ï¼Œå› æ­¤è¿›ä¸€æ­¥éªŒè¯äº†æ®‹å·®æ–¹å·®çš„é½æ€§ã€‚
    
---

### å½±å“æµ‹è¯•

> æ ·æœ¬å½±å“æ£€éªŒæ–¹æ³•

å¦‚æœæœ‰ä¸€äº›ç¦»æ•£ç‚¹è¿œç¦»å¤§éƒ¨åˆ†æ•°æ®ï¼Œé‚£ä¹ˆæ‹Ÿåˆå‡ºæ¥çš„æ¨¡å‹å¯èƒ½å°±ä¼šåç¦»æ­£å¸¸è½¨è¿¹å—åˆ°å½±å“ã€‚
å› æ­¤ï¼Œåœ¨åšçº¿æ€§å›å½’è¯Šæ–­åˆ†æçš„æ—¶å€™ä¹Ÿå¿…é¡»æŠŠè¿™äº›å¼ºå½±å“ç‚¹è€ƒè™‘è¿›å»ï¼Œè¿›è¡Œåˆ†æã€‚
é’ˆå¯¹äºå¼ºå½±å“ç‚¹åˆ†æï¼Œä¸€èˆ¬çš„æœ‰ä»¥ä¸‹å‡ ç§æ–¹æ³•ï¼š

1. å­¦ç”ŸåŒ–æ®‹å·®`SR`: æŒ‡æ®‹å·®æ ‡å‡†åŒ–åçš„æ•°å€¼ã€‚ä¸€èˆ¬çš„å½“æ ·æœ¬é‡ä¸ºå‡ ç™¾æ—¶ï¼Œ
å­¦ç”ŸåŒ–æ®‹å·®å¤§äº`2`çš„ç‚¹è¢«è§†ä¸ºå¼ºå½±å“ç‚¹ï¼Œè€Œå½“æ ·æœ¬é‡ä¸ºä¸Šåƒæ—¶ï¼Œ
å­¦ç”ŸåŒ–æ®‹å·®ä¸­å¤§äº`3`çš„ç‚¹ä¸ºç›¸å¯¹å¤§çš„å½±å“ç‚¹ã€‚

2. `Cook's D`ç»Ÿè®¡é‡: ç”¨äºæµ‹é‡å½“ç¬¬`i`ä¸ªè§‚æµ‹å€¼ä»åˆ†æä¸­å»é™¤æ—¶ï¼Œå‚æ•°ä¼°è®¡çš„æ”¹å˜ç¨‹åº¦ã€‚
ä¸€èˆ¬çš„`Cook's D`å€¼è¶Šå¤§è¯´æ˜è¶Šå¯èƒ½æ˜¯ç¦»æ•£ç‚¹ï¼Œæ²¡æœ‰å¾ˆæ˜ç¡®çš„ä¸´ç•Œå€¼ã€‚
å»ºè®®çš„å½±å“ä¸´ç•Œç‚¹æ˜¯: $Cook's\ D > \frac{4}{n}$ï¼Œå³é«˜äºæ­¤å€¼å¯è¢«è§†ä¸ºå¼ºå½±å“ç‚¹ã€‚

$$
Distance_i = \frac{1}{p+1} \frac{h_{ii}}{1-h_{ii}} r_i^2
$$

3. `DFFITS`ç»Ÿè®¡é‡: ç”¨äºæµ‹é‡ç¬¬`i`ä¸ªè§‚æµ‹å€¼å¯¹é¢„æµ‹å€¼çš„å½±å“ã€‚å»ºè®®çš„ä¸´ç•Œå€¼ä¸º: $|DFFITS_i|>2\sqrt{\frac{p}{n}}$

4. `DFBETAS`ç»Ÿè®¡é‡: ç”¨äºæµ‹é‡å½“å»é™¤ç¬¬`i`ä¸ªè§‚æµ‹é‡æ—¶ï¼Œç¬¬`j`ä¸ªå‚æ•°ä¼°è®¡çš„å˜åŒ–ç¨‹åº¦ã€‚å»ºè®®çš„å½±å“ä¸´ç•Œå€¼ä¸º: $|DFBETAS_{ij}| > 2 \sqrt{\frac{1}{n}}$

> æ ·æœ¬å½±å“æ£€éªŒçš„å®ç°

1. ä»¥ä¸Šæ€»ä½“å‚è€ƒæŒ‡æ ‡ï¼Œè¯·å‚è€ƒ: [æœ€å°äºŒä¹˜ - å›å½’è¯Šæ–­](./machine_learning/linearregression?id=æ™®é€šæœ€å°äºŒä¹˜-ğŸ”¨)
2. å•ä¸ªæŒ‡æ ‡æŸ¥çœ‹

```python
influence = lr.res.model.get_influence()

leverage = influence.hat_diag_factor
dffits = influence.dffits
resid_stu = influence.resid_studentized_external
cook = influence.cooks_distance
```

> leverage

**leverage**æ˜¯è¡¡é‡ä¸€ä¸ªè§‚å¯Ÿå€¼çš„ç‹¬ç«‹å˜é‡å€¼ä¸å…¶ä»–è§‚å¯Ÿå€¼çš„ç›¸è·å¤šè¿œçš„åº¦é‡ã€‚
é«˜æ æ†ç‚¹`(High-leverage points)`æ˜¯æŒ‡åœ¨ç‹¬ç«‹å˜é‡çš„æå€¼æˆ–åå€¼å¤„è¿›è¡Œçš„é‚£äº›è§‚æµ‹ï¼Œ
å› æ­¤ç¼ºå°‘ç›¸é‚»è§‚æµ‹å€¼æ„å‘³ç€æ‹Ÿåˆçš„å›å½’æ¨¡å‹å°†é€šè¿‡è¯¥ç‰¹å®šè§‚æµ‹å€¼ï¼Œé€ æˆæ¨¡å‹æ‹Ÿåˆåå·®ã€‚

```python
lr.plot_leverage_resid2(lr.res)
plt.savefig('leverage_resid2.png', dpi=100)
```

<img src="./img/leverage_resid2.png" width="70%">

ç»˜åˆ¶`leverage`ä¸æ ‡å‡†åŒ–æ®‹å·®å¹³æ–¹çš„å…³ç³»ã€‚å…·æœ‰å¤§æ ‡å‡†åŒ–æ®‹å·®çš„è§‚æµ‹å€¼å°†åœ¨å›¾ä¸­æ ‡å‡ºã€‚

> è§£å†³æ–¹æ¡ˆ: è¯·å‚è€ƒ[RLMå›å½’](http://localhost:3000/#/./machine_learning/linearregression?id=rlmå›å½’)

### ğŸ”¨ çº¿æ€§æ€§

---


- [1] [Leverage (statistics)](https://en.wikipedia.org/wiki/Leverage_(statistics))
- [2] [Regression diagnostic](http://www.statsmodels.org/stable/diagnostic.html)
- [3] [å¤§è¯çº¿æ€§å›å½’](https://blog.csdn.net/lsxxx2011/article/details/98764818?ops_request_misc=%7B%22request%5Fid%22%3A%22158216255719724845038711%22%2C%22scm%22%3A%2220140713.130056874..%22%7D&request_id=158216255719724845038711&biz_id=0&utm_source=distribute.pc_search_result.none-task)
> Editor&Coding: Chensy

